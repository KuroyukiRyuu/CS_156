{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>CS156 (Introduction to AI), Spring 2022</b>\n",
    "# <u><b>Homework 10 submission</b></u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Roster Name: Benjamin Wu\n",
    "### Student ID: 013607880\n",
    "### Email address: benjamin.wu01@sjsu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <u>Solution</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, setup random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from numpy import expand_dims\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy.random import rand\n",
    "from numpy.random import randint\n",
    "from numpy.random import randn\n",
    "from numpy import vstack\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <u>References and sources </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all your references and sources here.\n",
    "This includes all sites/discussion boards/blogs/posts/etc. where you grabbed some code examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 28, 28, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "x = np.concatenate((x_train, x_test))\n",
    "y = np.concatenate((y_train, y_test))\n",
    "\n",
    "x = expand_dims(x, axis=-1)\n",
    "\n",
    "x = x.astype(\"float32\") / 255\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 14, 14, 64)        640       \n",
      "                                                                 \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 7, 7, 64)          102464    \n",
      "                                                                 \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 3137      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 143,169\n",
      "Trainable params: 143,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(28,28,1)):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same', input_shape=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (3,3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(64, (5,5), strides=(1, 1), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# define the discriminator model\n",
    "discriminator = define_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 6272)              633472    \n",
      "                                                                 \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 6272)              0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 14, 14, 128)      262272    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_4 (Conv2DT  (None, 14, 14, 128)      16512     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_5 (Conv2DT  (None, 28, 28, 128)      262272    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 28, 28, 128)       0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 28, 28, 1)         6273      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,180,801\n",
      "Trainable params: 1,180,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    model = Sequential()\n",
    "    # foundation for 7x7 image\n",
    "    n_nodes = 128 * 7 * 7\n",
    "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    # upsample to 14x14\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(128, (1,1), strides=(1,1), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsample to 28x28\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same'))\n",
    "    return model\n",
    " \n",
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# define the generator model\n",
    "generator = define_generator(latent_dim)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_4 (Sequential)   (None, 28, 28, 1)         1180801   \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 1)                 143169    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,323,970\n",
      "Trainable params: 1,180,801\n",
      "Non-trainable params: 143,169\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(g_model)\n",
    "    # add the discriminator\n",
    "    model.add(d_model)\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X = dataset[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = g_model.predict(x_input)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
    "    # prepare real samples\n",
    "    X_real, y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    #save_plot(x_fake, epoch)\n",
    "    # save the generator model tile file\n",
    "    #filename = 'generator_model_%03d.h5' % (epoch + 1)\n",
    "    #g_model.save(filename)  # serializing the model: https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "    \n",
    "# train the generator and discriminator together\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=256):\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # create training set for the discriminator\n",
    "            X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))\n",
    "            # update discriminator model weights\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            # prepare points in latent space as input for the generator\n",
    "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            # summarize loss on this batch\n",
    "            print('>%d, %d/%d, d_loss=%.3f, g_loss=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
    "        # evaluate the model performance, sometimes\n",
    "        #if (i+1) % 10 == 0:\n",
    "    summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
    "            \n",
    "    return g_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, 1/273, d_loss=0.688, g_loss=0.694\n",
      ">1, 2/273, d_loss=0.687, g_loss=0.714\n",
      ">1, 3/273, d_loss=0.677, g_loss=0.722\n",
      ">1, 4/273, d_loss=0.677, g_loss=0.741\n",
      ">1, 5/273, d_loss=0.664, g_loss=0.753\n",
      ">1, 6/273, d_loss=0.662, g_loss=0.777\n",
      ">1, 7/273, d_loss=0.656, g_loss=0.798\n",
      ">1, 8/273, d_loss=0.645, g_loss=0.808\n",
      ">1, 9/273, d_loss=0.637, g_loss=0.819\n",
      ">1, 10/273, d_loss=0.630, g_loss=0.825\n",
      ">1, 11/273, d_loss=0.614, g_loss=0.823\n",
      ">1, 12/273, d_loss=0.599, g_loss=0.809\n",
      ">1, 13/273, d_loss=0.587, g_loss=0.791\n",
      ">1, 14/273, d_loss=0.569, g_loss=0.765\n",
      ">1, 15/273, d_loss=0.541, g_loss=0.740\n",
      ">1, 16/273, d_loss=0.540, g_loss=0.717\n",
      ">1, 17/273, d_loss=0.530, g_loss=0.725\n",
      ">1, 18/273, d_loss=0.521, g_loss=0.782\n",
      ">1, 19/273, d_loss=0.482, g_loss=0.875\n",
      ">1, 20/273, d_loss=0.469, g_loss=0.959\n",
      ">1, 21/273, d_loss=0.461, g_loss=0.939\n",
      ">1, 22/273, d_loss=0.459, g_loss=0.872\n",
      ">1, 23/273, d_loss=0.461, g_loss=0.794\n",
      ">1, 24/273, d_loss=0.437, g_loss=0.738\n",
      ">1, 25/273, d_loss=0.442, g_loss=0.726\n",
      ">1, 26/273, d_loss=0.458, g_loss=0.710\n",
      ">1, 27/273, d_loss=0.468, g_loss=0.766\n",
      ">1, 28/273, d_loss=0.463, g_loss=0.833\n",
      ">1, 29/273, d_loss=0.470, g_loss=0.806\n",
      ">1, 30/273, d_loss=0.489, g_loss=0.724\n",
      ">1, 31/273, d_loss=0.504, g_loss=0.648\n",
      ">1, 32/273, d_loss=0.520, g_loss=0.637\n",
      ">1, 33/273, d_loss=0.543, g_loss=0.694\n",
      ">1, 34/273, d_loss=0.532, g_loss=0.709\n",
      ">1, 35/273, d_loss=0.560, g_loss=0.689\n",
      ">1, 36/273, d_loss=0.578, g_loss=0.625\n",
      ">1, 37/273, d_loss=0.561, g_loss=0.627\n",
      ">1, 38/273, d_loss=0.601, g_loss=0.712\n",
      ">1, 39/273, d_loss=0.601, g_loss=0.796\n",
      ">1, 40/273, d_loss=0.639, g_loss=0.663\n",
      ">1, 41/273, d_loss=0.683, g_loss=0.582\n",
      ">1, 42/273, d_loss=0.785, g_loss=0.681\n",
      ">1, 43/273, d_loss=0.784, g_loss=0.706\n",
      ">1, 44/273, d_loss=0.826, g_loss=0.631\n",
      ">1, 45/273, d_loss=0.776, g_loss=0.624\n",
      ">1, 46/273, d_loss=0.752, g_loss=0.635\n",
      ">1, 47/273, d_loss=0.710, g_loss=0.671\n",
      ">1, 48/273, d_loss=0.667, g_loss=0.693\n",
      ">1, 49/273, d_loss=0.634, g_loss=0.709\n",
      ">1, 50/273, d_loss=0.629, g_loss=0.729\n",
      ">1, 51/273, d_loss=0.619, g_loss=0.789\n",
      ">1, 52/273, d_loss=0.598, g_loss=0.897\n",
      ">1, 53/273, d_loss=0.608, g_loss=0.909\n",
      ">1, 54/273, d_loss=0.650, g_loss=0.948\n",
      ">1, 55/273, d_loss=0.649, g_loss=0.997\n",
      ">1, 56/273, d_loss=0.668, g_loss=0.894\n",
      ">1, 57/273, d_loss=0.684, g_loss=1.080\n",
      ">1, 58/273, d_loss=0.676, g_loss=1.017\n",
      ">1, 59/273, d_loss=0.684, g_loss=0.988\n",
      ">1, 60/273, d_loss=0.676, g_loss=0.902\n",
      ">1, 61/273, d_loss=0.667, g_loss=1.009\n",
      ">1, 62/273, d_loss=0.651, g_loss=0.929\n",
      ">1, 63/273, d_loss=0.643, g_loss=0.892\n",
      ">1, 64/273, d_loss=0.631, g_loss=0.923\n",
      ">1, 65/273, d_loss=0.652, g_loss=0.987\n",
      ">1, 66/273, d_loss=0.626, g_loss=0.889\n",
      ">1, 67/273, d_loss=0.610, g_loss=0.857\n",
      ">1, 68/273, d_loss=0.592, g_loss=0.858\n",
      ">1, 69/273, d_loss=0.599, g_loss=0.797\n",
      ">1, 70/273, d_loss=0.573, g_loss=0.760\n",
      ">1, 71/273, d_loss=0.566, g_loss=0.806\n",
      ">1, 72/273, d_loss=0.542, g_loss=0.824\n",
      ">1, 73/273, d_loss=0.521, g_loss=0.827\n",
      ">1, 74/273, d_loss=0.545, g_loss=0.873\n",
      ">1, 75/273, d_loss=0.521, g_loss=0.912\n",
      ">1, 76/273, d_loss=0.512, g_loss=0.976\n",
      ">1, 77/273, d_loss=0.542, g_loss=0.991\n",
      ">1, 78/273, d_loss=0.554, g_loss=1.128\n",
      ">1, 79/273, d_loss=0.612, g_loss=1.038\n",
      ">1, 80/273, d_loss=0.681, g_loss=1.093\n",
      ">1, 81/273, d_loss=0.709, g_loss=0.932\n",
      ">1, 82/273, d_loss=0.745, g_loss=1.102\n",
      ">1, 83/273, d_loss=0.759, g_loss=0.951\n",
      ">1, 84/273, d_loss=0.823, g_loss=0.712\n",
      ">1, 85/273, d_loss=0.870, g_loss=0.719\n",
      ">1, 86/273, d_loss=0.857, g_loss=0.785\n",
      ">1, 87/273, d_loss=0.755, g_loss=0.748\n",
      ">1, 88/273, d_loss=0.745, g_loss=0.672\n",
      ">1, 89/273, d_loss=0.703, g_loss=0.632\n",
      ">1, 90/273, d_loss=0.643, g_loss=0.606\n",
      ">1, 91/273, d_loss=0.617, g_loss=0.649\n",
      ">1, 92/273, d_loss=0.610, g_loss=0.666\n",
      ">1, 93/273, d_loss=0.622, g_loss=0.674\n",
      ">1, 94/273, d_loss=0.588, g_loss=0.689\n",
      ">1, 95/273, d_loss=0.575, g_loss=0.681\n",
      ">1, 96/273, d_loss=0.610, g_loss=0.647\n",
      ">1, 97/273, d_loss=0.586, g_loss=0.648\n",
      ">1, 98/273, d_loss=0.587, g_loss=0.664\n",
      ">1, 99/273, d_loss=0.578, g_loss=0.688\n",
      ">1, 100/273, d_loss=0.582, g_loss=0.705\n",
      ">1, 101/273, d_loss=0.593, g_loss=0.728\n",
      ">1, 102/273, d_loss=0.600, g_loss=0.757\n",
      ">1, 103/273, d_loss=0.583, g_loss=0.783\n",
      ">1, 104/273, d_loss=0.605, g_loss=0.790\n",
      ">1, 105/273, d_loss=0.612, g_loss=0.827\n",
      ">1, 106/273, d_loss=0.601, g_loss=0.888\n",
      ">1, 107/273, d_loss=0.628, g_loss=0.880\n",
      ">1, 108/273, d_loss=0.627, g_loss=0.883\n",
      ">1, 109/273, d_loss=0.658, g_loss=0.972\n",
      ">1, 110/273, d_loss=0.648, g_loss=0.976\n",
      ">1, 111/273, d_loss=0.659, g_loss=0.957\n",
      ">1, 112/273, d_loss=0.683, g_loss=0.891\n",
      ">1, 113/273, d_loss=0.661, g_loss=0.876\n",
      ">1, 114/273, d_loss=0.658, g_loss=0.833\n",
      ">1, 115/273, d_loss=0.660, g_loss=0.878\n",
      ">1, 116/273, d_loss=0.658, g_loss=0.845\n",
      ">1, 117/273, d_loss=0.656, g_loss=0.853\n",
      ">1, 118/273, d_loss=0.685, g_loss=0.783\n",
      ">1, 119/273, d_loss=0.698, g_loss=0.819\n",
      ">1, 120/273, d_loss=0.702, g_loss=0.884\n",
      ">1, 121/273, d_loss=0.694, g_loss=0.891\n",
      ">1, 122/273, d_loss=0.715, g_loss=0.888\n",
      ">1, 123/273, d_loss=0.708, g_loss=0.841\n",
      ">1, 124/273, d_loss=0.716, g_loss=0.858\n",
      ">1, 125/273, d_loss=0.698, g_loss=0.838\n",
      ">1, 126/273, d_loss=0.682, g_loss=0.885\n",
      ">1, 127/273, d_loss=0.673, g_loss=0.942\n",
      ">1, 128/273, d_loss=0.670, g_loss=0.926\n",
      ">1, 129/273, d_loss=0.656, g_loss=0.896\n",
      ">1, 130/273, d_loss=0.656, g_loss=0.861\n",
      ">1, 131/273, d_loss=0.638, g_loss=0.869\n",
      ">1, 132/273, d_loss=0.624, g_loss=0.849\n",
      ">1, 133/273, d_loss=0.637, g_loss=0.870\n",
      ">1, 134/273, d_loss=0.628, g_loss=0.843\n",
      ">1, 135/273, d_loss=0.627, g_loss=0.836\n",
      ">1, 136/273, d_loss=0.647, g_loss=0.827\n",
      ">1, 137/273, d_loss=0.634, g_loss=0.757\n",
      ">1, 138/273, d_loss=0.646, g_loss=0.730\n",
      ">1, 139/273, d_loss=0.625, g_loss=0.746\n",
      ">1, 140/273, d_loss=0.646, g_loss=0.702\n",
      ">1, 141/273, d_loss=0.654, g_loss=0.705\n",
      ">1, 142/273, d_loss=0.646, g_loss=0.680\n",
      ">1, 143/273, d_loss=0.660, g_loss=0.724\n",
      ">1, 144/273, d_loss=0.654, g_loss=0.816\n",
      ">1, 145/273, d_loss=0.673, g_loss=0.851\n",
      ">1, 146/273, d_loss=0.666, g_loss=0.853\n",
      ">1, 147/273, d_loss=0.674, g_loss=0.841\n",
      ">1, 148/273, d_loss=0.669, g_loss=0.883\n",
      ">1, 149/273, d_loss=0.672, g_loss=0.948\n",
      ">1, 150/273, d_loss=0.675, g_loss=0.914\n",
      ">1, 151/273, d_loss=0.687, g_loss=0.906\n",
      ">1, 152/273, d_loss=0.672, g_loss=0.871\n",
      ">1, 153/273, d_loss=0.674, g_loss=0.804\n",
      ">1, 154/273, d_loss=0.679, g_loss=0.879\n",
      ">1, 155/273, d_loss=0.679, g_loss=0.876\n",
      ">1, 156/273, d_loss=0.665, g_loss=0.869\n",
      ">1, 157/273, d_loss=0.654, g_loss=0.848\n",
      ">1, 158/273, d_loss=0.680, g_loss=0.768\n",
      ">1, 159/273, d_loss=0.685, g_loss=0.731\n",
      ">1, 160/273, d_loss=0.677, g_loss=0.718\n",
      ">1, 161/273, d_loss=0.694, g_loss=0.786\n",
      ">1, 162/273, d_loss=0.689, g_loss=0.770\n",
      ">1, 163/273, d_loss=0.713, g_loss=0.785\n",
      ">1, 164/273, d_loss=0.690, g_loss=0.716\n",
      ">1, 165/273, d_loss=0.720, g_loss=0.700\n",
      ">1, 166/273, d_loss=0.704, g_loss=0.693\n",
      ">1, 167/273, d_loss=0.701, g_loss=0.705\n",
      ">1, 168/273, d_loss=0.708, g_loss=0.728\n",
      ">1, 169/273, d_loss=0.694, g_loss=0.711\n",
      ">1, 170/273, d_loss=0.708, g_loss=0.722\n",
      ">1, 171/273, d_loss=0.688, g_loss=0.733\n",
      ">1, 172/273, d_loss=0.689, g_loss=0.795\n",
      ">1, 173/273, d_loss=0.687, g_loss=0.839\n",
      ">1, 174/273, d_loss=0.661, g_loss=0.804\n",
      ">1, 175/273, d_loss=0.678, g_loss=0.780\n",
      ">1, 176/273, d_loss=0.673, g_loss=0.816\n",
      ">1, 177/273, d_loss=0.678, g_loss=0.788\n",
      ">1, 178/273, d_loss=0.641, g_loss=0.725\n",
      ">1, 179/273, d_loss=0.661, g_loss=0.717\n",
      ">1, 180/273, d_loss=0.678, g_loss=0.753\n",
      ">1, 181/273, d_loss=0.668, g_loss=0.759\n",
      ">1, 182/273, d_loss=0.686, g_loss=0.755\n",
      ">1, 183/273, d_loss=0.691, g_loss=0.797\n",
      ">1, 184/273, d_loss=0.682, g_loss=0.790\n",
      ">1, 185/273, d_loss=0.689, g_loss=0.761\n",
      ">1, 186/273, d_loss=0.696, g_loss=0.752\n",
      ">1, 187/273, d_loss=0.689, g_loss=0.711\n",
      ">1, 188/273, d_loss=0.701, g_loss=0.724\n",
      ">1, 189/273, d_loss=0.706, g_loss=0.724\n",
      ">1, 190/273, d_loss=0.686, g_loss=0.783\n",
      ">1, 191/273, d_loss=0.704, g_loss=0.815\n",
      ">1, 192/273, d_loss=0.686, g_loss=0.856\n",
      ">1, 193/273, d_loss=0.703, g_loss=0.843\n",
      ">1, 194/273, d_loss=0.693, g_loss=0.766\n",
      ">1, 195/273, d_loss=0.683, g_loss=0.746\n",
      ">1, 196/273, d_loss=0.693, g_loss=0.756\n",
      ">1, 197/273, d_loss=0.671, g_loss=0.746\n",
      ">1, 198/273, d_loss=0.696, g_loss=0.729\n",
      ">1, 199/273, d_loss=0.690, g_loss=0.723\n",
      ">1, 200/273, d_loss=0.674, g_loss=0.777\n",
      ">1, 201/273, d_loss=0.675, g_loss=0.775\n",
      ">1, 202/273, d_loss=0.674, g_loss=0.762\n",
      ">1, 203/273, d_loss=0.679, g_loss=0.764\n",
      ">1, 204/273, d_loss=0.681, g_loss=0.749\n",
      ">1, 205/273, d_loss=0.678, g_loss=0.740\n",
      ">1, 206/273, d_loss=0.681, g_loss=0.722\n",
      ">1, 207/273, d_loss=0.689, g_loss=0.713\n",
      ">1, 208/273, d_loss=0.677, g_loss=0.714\n",
      ">1, 209/273, d_loss=0.677, g_loss=0.726\n",
      ">1, 210/273, d_loss=0.681, g_loss=0.732\n",
      ">1, 211/273, d_loss=0.677, g_loss=0.732\n",
      ">1, 212/273, d_loss=0.667, g_loss=0.747\n",
      ">1, 213/273, d_loss=0.678, g_loss=0.760\n",
      ">1, 214/273, d_loss=0.671, g_loss=0.781\n",
      ">1, 215/273, d_loss=0.663, g_loss=0.775\n",
      ">1, 216/273, d_loss=0.690, g_loss=0.796\n",
      ">1, 217/273, d_loss=0.657, g_loss=0.802\n",
      ">1, 218/273, d_loss=0.666, g_loss=0.816\n",
      ">1, 219/273, d_loss=0.666, g_loss=0.860\n",
      ">1, 220/273, d_loss=0.661, g_loss=0.876\n",
      ">1, 221/273, d_loss=0.684, g_loss=0.895\n",
      ">1, 222/273, d_loss=0.666, g_loss=0.808\n",
      ">1, 223/273, d_loss=0.686, g_loss=0.735\n",
      ">1, 224/273, d_loss=0.680, g_loss=0.737\n",
      ">1, 225/273, d_loss=0.681, g_loss=0.704\n",
      ">1, 226/273, d_loss=0.696, g_loss=0.737\n",
      ">1, 227/273, d_loss=0.687, g_loss=0.732\n",
      ">1, 228/273, d_loss=0.689, g_loss=0.723\n",
      ">1, 229/273, d_loss=0.702, g_loss=0.730\n",
      ">1, 230/273, d_loss=0.704, g_loss=0.712\n",
      ">1, 231/273, d_loss=0.696, g_loss=0.706\n",
      ">1, 232/273, d_loss=0.702, g_loss=0.683\n",
      ">1, 233/273, d_loss=0.718, g_loss=0.711\n",
      ">1, 234/273, d_loss=0.705, g_loss=0.694\n",
      ">1, 235/273, d_loss=0.695, g_loss=0.683\n",
      ">1, 236/273, d_loss=0.705, g_loss=0.689\n",
      ">1, 237/273, d_loss=0.690, g_loss=0.690\n",
      ">1, 238/273, d_loss=0.690, g_loss=0.695\n",
      ">1, 239/273, d_loss=0.671, g_loss=0.684\n",
      ">1, 240/273, d_loss=0.677, g_loss=0.694\n",
      ">1, 241/273, d_loss=0.677, g_loss=0.687\n",
      ">1, 242/273, d_loss=0.676, g_loss=0.707\n",
      ">1, 243/273, d_loss=0.670, g_loss=0.726\n",
      ">1, 244/273, d_loss=0.670, g_loss=0.771\n",
      ">1, 245/273, d_loss=0.665, g_loss=0.746\n",
      ">1, 246/273, d_loss=0.672, g_loss=0.749\n",
      ">1, 247/273, d_loss=0.673, g_loss=0.721\n",
      ">1, 248/273, d_loss=0.677, g_loss=0.698\n",
      ">1, 249/273, d_loss=0.681, g_loss=0.685\n",
      ">1, 250/273, d_loss=0.682, g_loss=0.717\n",
      ">1, 251/273, d_loss=0.684, g_loss=0.712\n",
      ">1, 252/273, d_loss=0.685, g_loss=0.698\n",
      ">1, 253/273, d_loss=0.687, g_loss=0.706\n",
      ">1, 254/273, d_loss=0.698, g_loss=0.738\n",
      ">1, 255/273, d_loss=0.694, g_loss=0.747\n",
      ">1, 256/273, d_loss=0.699, g_loss=0.692\n",
      ">1, 257/273, d_loss=0.702, g_loss=0.689\n",
      ">1, 258/273, d_loss=0.691, g_loss=0.688\n",
      ">1, 259/273, d_loss=0.685, g_loss=0.696\n",
      ">1, 260/273, d_loss=0.690, g_loss=0.725\n",
      ">1, 261/273, d_loss=0.692, g_loss=0.732\n",
      ">1, 262/273, d_loss=0.685, g_loss=0.781\n",
      ">1, 263/273, d_loss=0.703, g_loss=0.754\n",
      ">1, 264/273, d_loss=0.700, g_loss=0.736\n",
      ">1, 265/273, d_loss=0.676, g_loss=0.726\n",
      ">1, 266/273, d_loss=0.684, g_loss=0.727\n",
      ">1, 267/273, d_loss=0.688, g_loss=0.726\n",
      ">1, 268/273, d_loss=0.691, g_loss=0.743\n",
      ">1, 269/273, d_loss=0.671, g_loss=0.749\n",
      ">1, 270/273, d_loss=0.676, g_loss=0.750\n",
      ">1, 271/273, d_loss=0.678, g_loss=0.774\n",
      ">1, 272/273, d_loss=0.672, g_loss=0.775\n",
      ">1, 273/273, d_loss=0.671, g_loss=0.763\n",
      ">2, 1/273, d_loss=0.661, g_loss=0.756\n",
      ">2, 2/273, d_loss=0.667, g_loss=0.721\n",
      ">2, 3/273, d_loss=0.658, g_loss=0.703\n",
      ">2, 4/273, d_loss=0.659, g_loss=0.697\n",
      ">2, 5/273, d_loss=0.666, g_loss=0.687\n",
      ">2, 6/273, d_loss=0.667, g_loss=0.676\n",
      ">2, 7/273, d_loss=0.658, g_loss=0.672\n",
      ">2, 8/273, d_loss=0.679, g_loss=0.685\n",
      ">2, 9/273, d_loss=0.673, g_loss=0.712\n",
      ">2, 10/273, d_loss=0.676, g_loss=0.729\n",
      ">2, 11/273, d_loss=0.692, g_loss=0.774\n",
      ">2, 12/273, d_loss=0.674, g_loss=0.774\n",
      ">2, 13/273, d_loss=0.692, g_loss=0.768\n",
      ">2, 14/273, d_loss=0.700, g_loss=0.730\n",
      ">2, 15/273, d_loss=0.696, g_loss=0.742\n",
      ">2, 16/273, d_loss=0.705, g_loss=0.721\n",
      ">2, 17/273, d_loss=0.709, g_loss=0.681\n",
      ">2, 18/273, d_loss=0.705, g_loss=0.691\n",
      ">2, 19/273, d_loss=0.718, g_loss=0.686\n",
      ">2, 20/273, d_loss=0.719, g_loss=0.682\n",
      ">2, 21/273, d_loss=0.710, g_loss=0.670\n",
      ">2, 22/273, d_loss=0.704, g_loss=0.689\n",
      ">2, 23/273, d_loss=0.696, g_loss=0.699\n",
      ">2, 24/273, d_loss=0.684, g_loss=0.709\n",
      ">2, 25/273, d_loss=0.685, g_loss=0.718\n",
      ">2, 26/273, d_loss=0.675, g_loss=0.754\n",
      ">2, 27/273, d_loss=0.673, g_loss=0.760\n",
      ">2, 28/273, d_loss=0.648, g_loss=0.754\n",
      ">2, 29/273, d_loss=0.648, g_loss=0.741\n",
      ">2, 30/273, d_loss=0.647, g_loss=0.743\n",
      ">2, 31/273, d_loss=0.650, g_loss=0.742\n",
      ">2, 32/273, d_loss=0.656, g_loss=0.781\n",
      ">2, 33/273, d_loss=0.667, g_loss=0.821\n",
      ">2, 34/273, d_loss=0.668, g_loss=0.837\n",
      ">2, 35/273, d_loss=0.669, g_loss=0.810\n",
      ">2, 36/273, d_loss=0.684, g_loss=0.770\n",
      ">2, 37/273, d_loss=0.677, g_loss=0.729\n",
      ">2, 38/273, d_loss=0.683, g_loss=0.715\n",
      ">2, 39/273, d_loss=0.682, g_loss=0.704\n",
      ">2, 40/273, d_loss=0.679, g_loss=0.703\n",
      ">2, 41/273, d_loss=0.687, g_loss=0.711\n",
      ">2, 42/273, d_loss=0.681, g_loss=0.703\n",
      ">2, 43/273, d_loss=0.679, g_loss=0.715\n",
      ">2, 44/273, d_loss=0.692, g_loss=0.728\n",
      ">2, 45/273, d_loss=0.685, g_loss=0.732\n",
      ">2, 46/273, d_loss=0.690, g_loss=0.736\n",
      ">2, 47/273, d_loss=0.699, g_loss=0.717\n",
      ">2, 48/273, d_loss=0.693, g_loss=0.728\n",
      ">2, 49/273, d_loss=0.700, g_loss=0.748\n",
      ">2, 50/273, d_loss=0.696, g_loss=0.775\n",
      ">2, 51/273, d_loss=0.687, g_loss=0.800\n",
      ">2, 52/273, d_loss=0.691, g_loss=0.797\n",
      ">2, 53/273, d_loss=0.683, g_loss=0.791\n",
      ">2, 54/273, d_loss=0.698, g_loss=0.748\n",
      ">2, 55/273, d_loss=0.685, g_loss=0.700\n",
      ">2, 56/273, d_loss=0.691, g_loss=0.703\n",
      ">2, 57/273, d_loss=0.680, g_loss=0.684\n",
      ">2, 58/273, d_loss=0.685, g_loss=0.687\n",
      ">2, 59/273, d_loss=0.689, g_loss=0.695\n",
      ">2, 60/273, d_loss=0.695, g_loss=0.701\n",
      ">2, 61/273, d_loss=0.695, g_loss=0.693\n",
      ">2, 62/273, d_loss=0.697, g_loss=0.667\n",
      ">2, 63/273, d_loss=0.687, g_loss=0.673\n",
      ">2, 64/273, d_loss=0.695, g_loss=0.675\n",
      ">2, 65/273, d_loss=0.683, g_loss=0.684\n",
      ">2, 66/273, d_loss=0.695, g_loss=0.691\n",
      ">2, 67/273, d_loss=0.685, g_loss=0.688\n",
      ">2, 68/273, d_loss=0.691, g_loss=0.694\n",
      ">2, 69/273, d_loss=0.693, g_loss=0.685\n",
      ">2, 70/273, d_loss=0.691, g_loss=0.676\n",
      ">2, 71/273, d_loss=0.691, g_loss=0.686\n",
      ">2, 72/273, d_loss=0.688, g_loss=0.729\n",
      ">2, 73/273, d_loss=0.690, g_loss=0.726\n",
      ">2, 74/273, d_loss=0.679, g_loss=0.746\n",
      ">2, 75/273, d_loss=0.674, g_loss=0.728\n",
      ">2, 76/273, d_loss=0.665, g_loss=0.744\n",
      ">2, 77/273, d_loss=0.677, g_loss=0.765\n",
      ">2, 78/273, d_loss=0.686, g_loss=0.770\n",
      ">2, 79/273, d_loss=0.684, g_loss=0.763\n",
      ">2, 80/273, d_loss=0.689, g_loss=0.741\n",
      ">2, 81/273, d_loss=0.696, g_loss=0.710\n",
      ">2, 82/273, d_loss=0.695, g_loss=0.672\n",
      ">2, 83/273, d_loss=0.693, g_loss=0.680\n",
      ">2, 84/273, d_loss=0.700, g_loss=0.688\n",
      ">2, 85/273, d_loss=0.691, g_loss=0.713\n",
      ">2, 86/273, d_loss=0.682, g_loss=0.752\n",
      ">2, 87/273, d_loss=0.691, g_loss=0.808\n",
      ">2, 88/273, d_loss=0.681, g_loss=0.854\n",
      ">2, 89/273, d_loss=0.674, g_loss=0.840\n",
      ">2, 90/273, d_loss=0.676, g_loss=0.819\n",
      ">2, 91/273, d_loss=0.691, g_loss=0.781\n",
      ">2, 92/273, d_loss=0.687, g_loss=0.759\n",
      ">2, 93/273, d_loss=0.677, g_loss=0.736\n",
      ">2, 94/273, d_loss=0.685, g_loss=0.740\n",
      ">2, 95/273, d_loss=0.694, g_loss=0.734\n",
      ">2, 96/273, d_loss=0.683, g_loss=0.764\n",
      ">2, 97/273, d_loss=0.686, g_loss=0.798\n",
      ">2, 98/273, d_loss=0.693, g_loss=0.786\n",
      ">2, 99/273, d_loss=0.683, g_loss=0.783\n",
      ">2, 100/273, d_loss=0.700, g_loss=0.734\n",
      ">2, 101/273, d_loss=0.706, g_loss=0.743\n",
      ">2, 102/273, d_loss=0.696, g_loss=0.732\n",
      ">2, 103/273, d_loss=0.697, g_loss=0.750\n",
      ">2, 104/273, d_loss=0.705, g_loss=0.745\n",
      ">2, 105/273, d_loss=0.701, g_loss=0.760\n",
      ">2, 106/273, d_loss=0.700, g_loss=0.792\n",
      ">2, 107/273, d_loss=0.702, g_loss=0.813\n",
      ">2, 108/273, d_loss=0.688, g_loss=0.829\n",
      ">2, 109/273, d_loss=0.678, g_loss=0.819\n",
      ">2, 110/273, d_loss=0.685, g_loss=0.816\n",
      ">2, 111/273, d_loss=0.683, g_loss=0.797\n",
      ">2, 112/273, d_loss=0.689, g_loss=0.772\n",
      ">2, 113/273, d_loss=0.668, g_loss=0.751\n",
      ">2, 114/273, d_loss=0.674, g_loss=0.739\n",
      ">2, 115/273, d_loss=0.677, g_loss=0.723\n",
      ">2, 116/273, d_loss=0.680, g_loss=0.744\n",
      ">2, 117/273, d_loss=0.672, g_loss=0.743\n",
      ">2, 118/273, d_loss=0.669, g_loss=0.751\n",
      ">2, 119/273, d_loss=0.675, g_loss=0.753\n",
      ">2, 120/273, d_loss=0.669, g_loss=0.746\n",
      ">2, 121/273, d_loss=0.682, g_loss=0.720\n",
      ">2, 122/273, d_loss=0.684, g_loss=0.745\n",
      ">2, 123/273, d_loss=0.679, g_loss=0.731\n",
      ">2, 124/273, d_loss=0.671, g_loss=0.736\n",
      ">2, 125/273, d_loss=0.682, g_loss=0.721\n",
      ">2, 126/273, d_loss=0.690, g_loss=0.723\n",
      ">2, 127/273, d_loss=0.701, g_loss=0.728\n",
      ">2, 128/273, d_loss=0.696, g_loss=0.735\n",
      ">2, 129/273, d_loss=0.700, g_loss=0.719\n",
      ">2, 130/273, d_loss=0.705, g_loss=0.695\n",
      ">2, 131/273, d_loss=0.708, g_loss=0.686\n",
      ">2, 132/273, d_loss=0.691, g_loss=0.677\n",
      ">2, 133/273, d_loss=0.704, g_loss=0.678\n",
      ">2, 134/273, d_loss=0.694, g_loss=0.672\n",
      ">2, 135/273, d_loss=0.691, g_loss=0.671\n",
      ">2, 136/273, d_loss=0.701, g_loss=0.676\n",
      ">2, 137/273, d_loss=0.693, g_loss=0.687\n",
      ">2, 138/273, d_loss=0.689, g_loss=0.694\n",
      ">2, 139/273, d_loss=0.680, g_loss=0.713\n",
      ">2, 140/273, d_loss=0.677, g_loss=0.727\n",
      ">2, 141/273, d_loss=0.672, g_loss=0.736\n",
      ">2, 142/273, d_loss=0.669, g_loss=0.741\n",
      ">2, 143/273, d_loss=0.657, g_loss=0.739\n",
      ">2, 144/273, d_loss=0.653, g_loss=0.736\n",
      ">2, 145/273, d_loss=0.652, g_loss=0.742\n",
      ">2, 146/273, d_loss=0.660, g_loss=0.734\n",
      ">2, 147/273, d_loss=0.648, g_loss=0.728\n",
      ">2, 148/273, d_loss=0.658, g_loss=0.723\n",
      ">2, 149/273, d_loss=0.666, g_loss=0.723\n",
      ">2, 150/273, d_loss=0.670, g_loss=0.748\n",
      ">2, 151/273, d_loss=0.665, g_loss=0.755\n",
      ">2, 152/273, d_loss=0.687, g_loss=0.760\n",
      ">2, 153/273, d_loss=0.681, g_loss=0.755\n",
      ">2, 154/273, d_loss=0.672, g_loss=0.749\n",
      ">2, 155/273, d_loss=0.678, g_loss=0.720\n",
      ">2, 156/273, d_loss=0.674, g_loss=0.706\n",
      ">2, 157/273, d_loss=0.679, g_loss=0.690\n",
      ">2, 158/273, d_loss=0.690, g_loss=0.712\n",
      ">2, 159/273, d_loss=0.679, g_loss=0.728\n",
      ">2, 160/273, d_loss=0.674, g_loss=0.732\n",
      ">2, 161/273, d_loss=0.689, g_loss=0.735\n",
      ">2, 162/273, d_loss=0.683, g_loss=0.724\n",
      ">2, 163/273, d_loss=0.687, g_loss=0.741\n",
      ">2, 164/273, d_loss=0.682, g_loss=0.720\n",
      ">2, 165/273, d_loss=0.692, g_loss=0.713\n",
      ">2, 166/273, d_loss=0.688, g_loss=0.667\n",
      ">2, 167/273, d_loss=0.685, g_loss=0.669\n",
      ">2, 168/273, d_loss=0.674, g_loss=0.675\n",
      ">2, 169/273, d_loss=0.692, g_loss=0.694\n",
      ">2, 170/273, d_loss=0.701, g_loss=0.741\n",
      ">2, 171/273, d_loss=0.683, g_loss=0.760\n",
      ">2, 172/273, d_loss=0.687, g_loss=0.782\n",
      ">2, 173/273, d_loss=0.680, g_loss=0.785\n",
      ">2, 174/273, d_loss=0.683, g_loss=0.755\n",
      ">2, 175/273, d_loss=0.677, g_loss=0.732\n",
      ">2, 176/273, d_loss=0.683, g_loss=0.699\n",
      ">2, 177/273, d_loss=0.687, g_loss=0.695\n",
      ">2, 178/273, d_loss=0.681, g_loss=0.689\n",
      ">2, 179/273, d_loss=0.682, g_loss=0.677\n",
      ">2, 180/273, d_loss=0.679, g_loss=0.694\n",
      ">2, 181/273, d_loss=0.677, g_loss=0.720\n",
      ">2, 182/273, d_loss=0.681, g_loss=0.719\n",
      ">2, 183/273, d_loss=0.673, g_loss=0.731\n",
      ">2, 184/273, d_loss=0.672, g_loss=0.755\n",
      ">2, 185/273, d_loss=0.683, g_loss=0.748\n",
      ">2, 186/273, d_loss=0.661, g_loss=0.732\n",
      ">2, 187/273, d_loss=0.670, g_loss=0.716\n",
      ">2, 188/273, d_loss=0.675, g_loss=0.711\n",
      ">2, 189/273, d_loss=0.687, g_loss=0.680\n",
      ">2, 190/273, d_loss=0.669, g_loss=0.671\n",
      ">2, 191/273, d_loss=0.671, g_loss=0.672\n",
      ">2, 192/273, d_loss=0.678, g_loss=0.684\n",
      ">2, 193/273, d_loss=0.674, g_loss=0.700\n",
      ">2, 194/273, d_loss=0.682, g_loss=0.707\n",
      ">2, 195/273, d_loss=0.673, g_loss=0.719\n",
      ">2, 196/273, d_loss=0.654, g_loss=0.751\n",
      ">2, 197/273, d_loss=0.665, g_loss=0.752\n",
      ">2, 198/273, d_loss=0.670, g_loss=0.745\n",
      ">2, 199/273, d_loss=0.668, g_loss=0.723\n",
      ">2, 200/273, d_loss=0.668, g_loss=0.719\n",
      ">2, 201/273, d_loss=0.657, g_loss=0.694\n",
      ">2, 202/273, d_loss=0.670, g_loss=0.701\n",
      ">2, 203/273, d_loss=0.672, g_loss=0.731\n",
      ">2, 204/273, d_loss=0.679, g_loss=0.712\n",
      ">2, 205/273, d_loss=0.680, g_loss=0.725\n",
      ">2, 206/273, d_loss=0.681, g_loss=0.732\n",
      ">2, 207/273, d_loss=0.683, g_loss=0.710\n",
      ">2, 208/273, d_loss=0.685, g_loss=0.698\n",
      ">2, 209/273, d_loss=0.675, g_loss=0.707\n",
      ">2, 210/273, d_loss=0.680, g_loss=0.713\n",
      ">2, 211/273, d_loss=0.690, g_loss=0.707\n",
      ">2, 212/273, d_loss=0.693, g_loss=0.715\n",
      ">2, 213/273, d_loss=0.691, g_loss=0.713\n",
      ">2, 214/273, d_loss=0.682, g_loss=0.708\n",
      ">2, 215/273, d_loss=0.686, g_loss=0.709\n",
      ">2, 216/273, d_loss=0.687, g_loss=0.723\n",
      ">2, 217/273, d_loss=0.681, g_loss=0.747\n",
      ">2, 218/273, d_loss=0.676, g_loss=0.760\n",
      ">2, 219/273, d_loss=0.664, g_loss=0.771\n",
      ">2, 220/273, d_loss=0.666, g_loss=0.753\n",
      ">2, 221/273, d_loss=0.660, g_loss=0.743\n",
      ">2, 222/273, d_loss=0.655, g_loss=0.744\n",
      ">2, 223/273, d_loss=0.652, g_loss=0.748\n",
      ">2, 224/273, d_loss=0.651, g_loss=0.755\n",
      ">2, 225/273, d_loss=0.645, g_loss=0.777\n",
      ">2, 226/273, d_loss=0.658, g_loss=0.790\n",
      ">2, 227/273, d_loss=0.644, g_loss=0.794\n",
      ">2, 228/273, d_loss=0.657, g_loss=0.780\n",
      ">2, 229/273, d_loss=0.671, g_loss=0.810\n",
      ">2, 230/273, d_loss=0.649, g_loss=0.797\n",
      ">2, 231/273, d_loss=0.673, g_loss=0.761\n",
      ">2, 232/273, d_loss=0.682, g_loss=0.721\n",
      ">2, 233/273, d_loss=0.705, g_loss=0.672\n",
      ">2, 234/273, d_loss=0.687, g_loss=0.679\n",
      ">2, 235/273, d_loss=0.694, g_loss=0.694\n",
      ">2, 236/273, d_loss=0.689, g_loss=0.705\n",
      ">2, 237/273, d_loss=0.684, g_loss=0.707\n",
      ">2, 238/273, d_loss=0.701, g_loss=0.694\n",
      ">2, 239/273, d_loss=0.674, g_loss=0.716\n",
      ">2, 240/273, d_loss=0.686, g_loss=0.739\n",
      ">2, 241/273, d_loss=0.682, g_loss=0.730\n",
      ">2, 242/273, d_loss=0.674, g_loss=0.728\n",
      ">2, 243/273, d_loss=0.670, g_loss=0.757\n",
      ">2, 244/273, d_loss=0.659, g_loss=0.804\n",
      ">2, 245/273, d_loss=0.644, g_loss=0.847\n",
      ">2, 246/273, d_loss=0.644, g_loss=0.876\n",
      ">2, 247/273, d_loss=0.627, g_loss=0.867\n",
      ">2, 248/273, d_loss=0.631, g_loss=0.832\n",
      ">2, 249/273, d_loss=0.620, g_loss=0.830\n",
      ">2, 250/273, d_loss=0.638, g_loss=0.764\n",
      ">2, 251/273, d_loss=0.632, g_loss=0.712\n",
      ">2, 252/273, d_loss=0.652, g_loss=0.707\n",
      ">2, 253/273, d_loss=0.643, g_loss=0.730\n",
      ">2, 254/273, d_loss=0.637, g_loss=0.733\n",
      ">2, 255/273, d_loss=0.660, g_loss=0.747\n",
      ">2, 256/273, d_loss=0.672, g_loss=0.747\n",
      ">2, 257/273, d_loss=0.674, g_loss=0.742\n",
      ">2, 258/273, d_loss=0.692, g_loss=0.707\n",
      ">2, 259/273, d_loss=0.692, g_loss=0.725\n",
      ">2, 260/273, d_loss=0.702, g_loss=0.771\n",
      ">2, 261/273, d_loss=0.702, g_loss=0.825\n",
      ">2, 262/273, d_loss=0.690, g_loss=0.802\n",
      ">2, 263/273, d_loss=0.711, g_loss=0.773\n",
      ">2, 264/273, d_loss=0.689, g_loss=0.718\n",
      ">2, 265/273, d_loss=0.671, g_loss=0.703\n",
      ">2, 266/273, d_loss=0.680, g_loss=0.747\n",
      ">2, 267/273, d_loss=0.682, g_loss=0.766\n",
      ">2, 268/273, d_loss=0.671, g_loss=0.800\n",
      ">2, 269/273, d_loss=0.687, g_loss=0.786\n",
      ">2, 270/273, d_loss=0.671, g_loss=0.733\n",
      ">2, 271/273, d_loss=0.668, g_loss=0.720\n",
      ">2, 272/273, d_loss=0.687, g_loss=0.757\n",
      ">2, 273/273, d_loss=0.667, g_loss=0.776\n",
      ">3, 1/273, d_loss=0.663, g_loss=0.795\n",
      ">3, 2/273, d_loss=0.676, g_loss=0.804\n",
      ">3, 3/273, d_loss=0.674, g_loss=0.780\n",
      ">3, 4/273, d_loss=0.661, g_loss=0.742\n",
      ">3, 5/273, d_loss=0.664, g_loss=0.735\n",
      ">3, 6/273, d_loss=0.671, g_loss=0.724\n",
      ">3, 7/273, d_loss=0.669, g_loss=0.743\n",
      ">3, 8/273, d_loss=0.661, g_loss=0.733\n",
      ">3, 9/273, d_loss=0.676, g_loss=0.730\n",
      ">3, 10/273, d_loss=0.664, g_loss=0.729\n",
      ">3, 11/273, d_loss=0.666, g_loss=0.750\n",
      ">3, 12/273, d_loss=0.666, g_loss=0.760\n",
      ">3, 13/273, d_loss=0.678, g_loss=0.741\n",
      ">3, 14/273, d_loss=0.663, g_loss=0.708\n",
      ">3, 15/273, d_loss=0.667, g_loss=0.703\n",
      ">3, 16/273, d_loss=0.674, g_loss=0.701\n",
      ">3, 17/273, d_loss=0.659, g_loss=0.717\n",
      ">3, 18/273, d_loss=0.678, g_loss=0.722\n",
      ">3, 19/273, d_loss=0.669, g_loss=0.717\n",
      ">3, 20/273, d_loss=0.671, g_loss=0.718\n",
      ">3, 21/273, d_loss=0.691, g_loss=0.716\n",
      ">3, 22/273, d_loss=0.681, g_loss=0.744\n",
      ">3, 23/273, d_loss=0.679, g_loss=0.738\n",
      ">3, 24/273, d_loss=0.678, g_loss=0.703\n",
      ">3, 25/273, d_loss=0.680, g_loss=0.718\n",
      ">3, 26/273, d_loss=0.686, g_loss=0.735\n",
      ">3, 27/273, d_loss=0.669, g_loss=0.751\n",
      ">3, 28/273, d_loss=0.668, g_loss=0.757\n",
      ">3, 29/273, d_loss=0.675, g_loss=0.816\n",
      ">3, 30/273, d_loss=0.663, g_loss=0.840\n",
      ">3, 31/273, d_loss=0.656, g_loss=0.870\n",
      ">3, 32/273, d_loss=0.657, g_loss=0.854\n",
      ">3, 33/273, d_loss=0.625, g_loss=0.854\n",
      ">3, 34/273, d_loss=0.648, g_loss=0.811\n",
      ">3, 35/273, d_loss=0.630, g_loss=0.780\n",
      ">3, 36/273, d_loss=0.673, g_loss=0.759\n",
      ">3, 37/273, d_loss=0.677, g_loss=0.746\n",
      ">3, 38/273, d_loss=0.686, g_loss=0.735\n",
      ">3, 39/273, d_loss=0.704, g_loss=0.770\n",
      ">3, 40/273, d_loss=0.709, g_loss=0.775\n",
      ">3, 41/273, d_loss=0.697, g_loss=0.804\n",
      ">3, 42/273, d_loss=0.703, g_loss=0.776\n",
      ">3, 43/273, d_loss=0.725, g_loss=0.742\n",
      ">3, 44/273, d_loss=0.725, g_loss=0.752\n",
      ">3, 45/273, d_loss=0.737, g_loss=0.741\n",
      ">3, 46/273, d_loss=0.731, g_loss=0.741\n",
      ">3, 47/273, d_loss=0.747, g_loss=0.737\n",
      ">3, 48/273, d_loss=0.732, g_loss=0.749\n",
      ">3, 49/273, d_loss=0.706, g_loss=0.760\n",
      ">3, 50/273, d_loss=0.700, g_loss=0.760\n",
      ">3, 51/273, d_loss=0.673, g_loss=0.775\n",
      ">3, 52/273, d_loss=0.653, g_loss=0.789\n",
      ">3, 53/273, d_loss=0.635, g_loss=0.792\n",
      ">3, 54/273, d_loss=0.627, g_loss=0.778\n",
      ">3, 55/273, d_loss=0.615, g_loss=0.768\n",
      ">3, 56/273, d_loss=0.616, g_loss=0.769\n",
      ">3, 57/273, d_loss=0.625, g_loss=0.736\n",
      ">3, 58/273, d_loss=0.643, g_loss=0.733\n",
      ">3, 59/273, d_loss=0.649, g_loss=0.773\n",
      ">3, 60/273, d_loss=0.669, g_loss=0.802\n",
      ">3, 61/273, d_loss=0.671, g_loss=0.818\n",
      ">3, 62/273, d_loss=0.680, g_loss=0.826\n",
      ">3, 63/273, d_loss=0.716, g_loss=0.755\n",
      ">3, 64/273, d_loss=0.726, g_loss=0.728\n",
      ">3, 65/273, d_loss=0.729, g_loss=0.691\n",
      ">3, 66/273, d_loss=0.713, g_loss=0.671\n",
      ">3, 67/273, d_loss=0.737, g_loss=0.680\n",
      ">3, 68/273, d_loss=0.703, g_loss=0.679\n",
      ">3, 69/273, d_loss=0.686, g_loss=0.684\n",
      ">3, 70/273, d_loss=0.674, g_loss=0.680\n",
      ">3, 71/273, d_loss=0.669, g_loss=0.729\n",
      ">3, 72/273, d_loss=0.672, g_loss=0.780\n",
      ">3, 73/273, d_loss=0.667, g_loss=0.790\n",
      ">3, 74/273, d_loss=0.669, g_loss=0.778\n",
      ">3, 75/273, d_loss=0.672, g_loss=0.785\n",
      ">3, 76/273, d_loss=0.678, g_loss=0.779\n",
      ">3, 77/273, d_loss=0.675, g_loss=0.822\n",
      ">3, 78/273, d_loss=0.672, g_loss=0.818\n",
      ">3, 79/273, d_loss=0.672, g_loss=0.794\n",
      ">3, 80/273, d_loss=0.690, g_loss=0.783\n",
      ">3, 81/273, d_loss=0.679, g_loss=0.733\n",
      ">3, 82/273, d_loss=0.672, g_loss=0.697\n",
      ">3, 83/273, d_loss=0.671, g_loss=0.689\n",
      ">3, 84/273, d_loss=0.657, g_loss=0.707\n",
      ">3, 85/273, d_loss=0.653, g_loss=0.733\n",
      ">3, 86/273, d_loss=0.659, g_loss=0.764\n",
      ">3, 87/273, d_loss=0.653, g_loss=0.793\n",
      ">3, 88/273, d_loss=0.649, g_loss=0.833\n",
      ">3, 89/273, d_loss=0.658, g_loss=0.829\n",
      ">3, 90/273, d_loss=0.639, g_loss=0.820\n",
      ">3, 91/273, d_loss=0.666, g_loss=0.818\n",
      ">3, 92/273, d_loss=0.664, g_loss=0.899\n",
      ">3, 93/273, d_loss=0.680, g_loss=0.903\n",
      ">3, 94/273, d_loss=0.660, g_loss=0.884\n",
      ">3, 95/273, d_loss=0.662, g_loss=0.878\n",
      ">3, 96/273, d_loss=0.661, g_loss=0.860\n",
      ">3, 97/273, d_loss=0.679, g_loss=0.784\n",
      ">3, 98/273, d_loss=0.681, g_loss=0.754\n",
      ">3, 99/273, d_loss=0.683, g_loss=0.720\n",
      ">3, 100/273, d_loss=0.682, g_loss=0.710\n",
      ">3, 101/273, d_loss=0.679, g_loss=0.698\n",
      ">3, 102/273, d_loss=0.696, g_loss=0.668\n",
      ">3, 103/273, d_loss=0.690, g_loss=0.662\n",
      ">3, 104/273, d_loss=0.708, g_loss=0.673\n",
      ">3, 105/273, d_loss=0.689, g_loss=0.696\n",
      ">3, 106/273, d_loss=0.677, g_loss=0.735\n",
      ">3, 107/273, d_loss=0.701, g_loss=0.759\n",
      ">3, 108/273, d_loss=0.678, g_loss=0.740\n",
      ">3, 109/273, d_loss=0.683, g_loss=0.756\n",
      ">3, 110/273, d_loss=0.692, g_loss=0.774\n",
      ">3, 111/273, d_loss=0.692, g_loss=0.771\n",
      ">3, 112/273, d_loss=0.685, g_loss=0.808\n",
      ">3, 113/273, d_loss=0.679, g_loss=0.811\n",
      ">3, 114/273, d_loss=0.689, g_loss=0.789\n",
      ">3, 115/273, d_loss=0.677, g_loss=0.755\n",
      ">3, 116/273, d_loss=0.679, g_loss=0.742\n",
      ">3, 117/273, d_loss=0.671, g_loss=0.733\n",
      ">3, 118/273, d_loss=0.672, g_loss=0.729\n",
      ">3, 119/273, d_loss=0.673, g_loss=0.730\n",
      ">3, 120/273, d_loss=0.667, g_loss=0.704\n",
      ">3, 121/273, d_loss=0.663, g_loss=0.711\n",
      ">3, 122/273, d_loss=0.668, g_loss=0.703\n",
      ">3, 123/273, d_loss=0.656, g_loss=0.725\n",
      ">3, 124/273, d_loss=0.661, g_loss=0.729\n",
      ">3, 125/273, d_loss=0.660, g_loss=0.720\n",
      ">3, 126/273, d_loss=0.658, g_loss=0.714\n",
      ">3, 127/273, d_loss=0.667, g_loss=0.729\n",
      ">3, 128/273, d_loss=0.665, g_loss=0.753\n",
      ">3, 129/273, d_loss=0.663, g_loss=0.749\n",
      ">3, 130/273, d_loss=0.670, g_loss=0.746\n",
      ">3, 131/273, d_loss=0.677, g_loss=0.747\n",
      ">3, 132/273, d_loss=0.699, g_loss=0.729\n",
      ">3, 133/273, d_loss=0.681, g_loss=0.704\n",
      ">3, 134/273, d_loss=0.687, g_loss=0.708\n",
      ">3, 135/273, d_loss=0.694, g_loss=0.691\n",
      ">3, 136/273, d_loss=0.693, g_loss=0.659\n",
      ">3, 137/273, d_loss=0.688, g_loss=0.691\n",
      ">3, 138/273, d_loss=0.680, g_loss=0.690\n",
      ">3, 139/273, d_loss=0.670, g_loss=0.695\n",
      ">3, 140/273, d_loss=0.682, g_loss=0.724\n",
      ">3, 141/273, d_loss=0.665, g_loss=0.772\n",
      ">3, 142/273, d_loss=0.676, g_loss=0.805\n",
      ">3, 143/273, d_loss=0.655, g_loss=0.797\n",
      ">3, 144/273, d_loss=0.662, g_loss=0.784\n",
      ">3, 145/273, d_loss=0.653, g_loss=0.787\n",
      ">3, 146/273, d_loss=0.657, g_loss=0.792\n",
      ">3, 147/273, d_loss=0.646, g_loss=0.776\n",
      ">3, 148/273, d_loss=0.645, g_loss=0.772\n",
      ">3, 149/273, d_loss=0.651, g_loss=0.760\n",
      ">3, 150/273, d_loss=0.647, g_loss=0.797\n",
      ">3, 151/273, d_loss=0.653, g_loss=0.789\n",
      ">3, 152/273, d_loss=0.657, g_loss=0.785\n",
      ">3, 153/273, d_loss=0.651, g_loss=0.787\n",
      ">3, 154/273, d_loss=0.654, g_loss=0.734\n",
      ">3, 155/273, d_loss=0.652, g_loss=0.744\n",
      ">3, 156/273, d_loss=0.686, g_loss=0.721\n",
      ">3, 157/273, d_loss=0.678, g_loss=0.721\n",
      ">3, 158/273, d_loss=0.662, g_loss=0.726\n",
      ">3, 159/273, d_loss=0.670, g_loss=0.700\n",
      ">3, 160/273, d_loss=0.675, g_loss=0.728\n",
      ">3, 161/273, d_loss=0.684, g_loss=0.749\n",
      ">3, 162/273, d_loss=0.663, g_loss=0.758\n",
      ">3, 163/273, d_loss=0.660, g_loss=0.759\n",
      ">3, 164/273, d_loss=0.672, g_loss=0.767\n",
      ">3, 165/273, d_loss=0.667, g_loss=0.748\n",
      ">3, 166/273, d_loss=0.678, g_loss=0.747\n",
      ">3, 167/273, d_loss=0.664, g_loss=0.728\n",
      ">3, 168/273, d_loss=0.681, g_loss=0.693\n",
      ">3, 169/273, d_loss=0.667, g_loss=0.676\n",
      ">3, 170/273, d_loss=0.665, g_loss=0.695\n",
      ">3, 171/273, d_loss=0.663, g_loss=0.718\n",
      ">3, 172/273, d_loss=0.673, g_loss=0.740\n",
      ">3, 173/273, d_loss=0.663, g_loss=0.776\n",
      ">3, 174/273, d_loss=0.664, g_loss=0.833\n",
      ">3, 175/273, d_loss=0.678, g_loss=0.803\n",
      ">3, 176/273, d_loss=0.647, g_loss=0.788\n",
      ">3, 177/273, d_loss=0.664, g_loss=0.781\n",
      ">3, 178/273, d_loss=0.676, g_loss=0.760\n",
      ">3, 179/273, d_loss=0.668, g_loss=0.800\n",
      ">3, 180/273, d_loss=0.682, g_loss=0.805\n",
      ">3, 181/273, d_loss=0.665, g_loss=0.774\n",
      ">3, 182/273, d_loss=0.675, g_loss=0.735\n",
      ">3, 183/273, d_loss=0.662, g_loss=0.713\n",
      ">3, 184/273, d_loss=0.657, g_loss=0.766\n",
      ">3, 185/273, d_loss=0.670, g_loss=0.810\n",
      ">3, 186/273, d_loss=0.672, g_loss=0.811\n",
      ">3, 187/273, d_loss=0.676, g_loss=0.763\n",
      ">3, 188/273, d_loss=0.681, g_loss=0.752\n",
      ">3, 189/273, d_loss=0.682, g_loss=0.798\n",
      ">3, 190/273, d_loss=0.690, g_loss=0.795\n",
      ">3, 191/273, d_loss=0.675, g_loss=0.764\n",
      ">3, 192/273, d_loss=0.678, g_loss=0.709\n",
      ">3, 193/273, d_loss=0.688, g_loss=0.724\n",
      ">3, 194/273, d_loss=0.688, g_loss=0.749\n",
      ">3, 195/273, d_loss=0.672, g_loss=0.782\n",
      ">3, 196/273, d_loss=0.665, g_loss=0.809\n",
      ">3, 197/273, d_loss=0.667, g_loss=0.811\n",
      ">3, 198/273, d_loss=0.654, g_loss=0.823\n",
      ">3, 199/273, d_loss=0.666, g_loss=0.764\n",
      ">3, 200/273, d_loss=0.670, g_loss=0.724\n",
      ">3, 201/273, d_loss=0.642, g_loss=0.701\n",
      ">3, 202/273, d_loss=0.653, g_loss=0.709\n",
      ">3, 203/273, d_loss=0.652, g_loss=0.732\n",
      ">3, 204/273, d_loss=0.654, g_loss=0.779\n",
      ">3, 205/273, d_loss=0.651, g_loss=0.807\n",
      ">3, 206/273, d_loss=0.651, g_loss=0.764\n",
      ">3, 207/273, d_loss=0.674, g_loss=0.763\n",
      ">3, 208/273, d_loss=0.673, g_loss=0.778\n",
      ">3, 209/273, d_loss=0.673, g_loss=0.756\n",
      ">3, 210/273, d_loss=0.674, g_loss=0.770\n",
      ">3, 211/273, d_loss=0.668, g_loss=0.706\n",
      ">3, 212/273, d_loss=0.692, g_loss=0.670\n",
      ">3, 213/273, d_loss=0.704, g_loss=0.655\n",
      ">3, 214/273, d_loss=0.686, g_loss=0.629\n",
      ">3, 215/273, d_loss=0.702, g_loss=0.645\n",
      ">3, 216/273, d_loss=0.693, g_loss=0.657\n",
      ">3, 217/273, d_loss=0.687, g_loss=0.679\n",
      ">3, 218/273, d_loss=0.685, g_loss=0.689\n",
      ">3, 219/273, d_loss=0.675, g_loss=0.738\n",
      ">3, 220/273, d_loss=0.668, g_loss=0.771\n",
      ">3, 221/273, d_loss=0.654, g_loss=0.746\n",
      ">3, 222/273, d_loss=0.646, g_loss=0.768\n",
      ">3, 223/273, d_loss=0.662, g_loss=0.802\n",
      ">3, 224/273, d_loss=0.659, g_loss=0.808\n",
      ">3, 225/273, d_loss=0.663, g_loss=0.792\n",
      ">3, 226/273, d_loss=0.688, g_loss=0.753\n",
      ">3, 227/273, d_loss=0.677, g_loss=0.752\n",
      ">3, 228/273, d_loss=0.684, g_loss=0.757\n",
      ">3, 229/273, d_loss=0.680, g_loss=0.827\n",
      ">3, 230/273, d_loss=0.685, g_loss=0.805\n",
      ">3, 231/273, d_loss=0.681, g_loss=0.784\n",
      ">3, 232/273, d_loss=0.670, g_loss=0.815\n",
      ">3, 233/273, d_loss=0.687, g_loss=0.826\n",
      ">3, 234/273, d_loss=0.672, g_loss=0.803\n",
      ">3, 235/273, d_loss=0.668, g_loss=0.790\n",
      ">3, 236/273, d_loss=0.682, g_loss=0.785\n",
      ">3, 237/273, d_loss=0.667, g_loss=0.784\n",
      ">3, 238/273, d_loss=0.658, g_loss=0.805\n",
      ">3, 239/273, d_loss=0.663, g_loss=0.827\n",
      ">3, 240/273, d_loss=0.646, g_loss=0.842\n",
      ">3, 241/273, d_loss=0.652, g_loss=0.860\n",
      ">3, 242/273, d_loss=0.635, g_loss=0.853\n",
      ">3, 243/273, d_loss=0.643, g_loss=0.841\n",
      ">3, 244/273, d_loss=0.660, g_loss=0.762\n",
      ">3, 245/273, d_loss=0.661, g_loss=0.713\n",
      ">3, 246/273, d_loss=0.668, g_loss=0.706\n",
      ">3, 247/273, d_loss=0.677, g_loss=0.744\n",
      ">3, 248/273, d_loss=0.662, g_loss=0.740\n",
      ">3, 249/273, d_loss=0.680, g_loss=0.717\n",
      ">3, 250/273, d_loss=0.655, g_loss=0.706\n",
      ">3, 251/273, d_loss=0.663, g_loss=0.709\n",
      ">3, 252/273, d_loss=0.674, g_loss=0.756\n",
      ">3, 253/273, d_loss=0.658, g_loss=0.776\n",
      ">3, 254/273, d_loss=0.655, g_loss=0.784\n",
      ">3, 255/273, d_loss=0.671, g_loss=0.750\n",
      ">3, 256/273, d_loss=0.679, g_loss=0.772\n",
      ">3, 257/273, d_loss=0.696, g_loss=0.747\n",
      ">3, 258/273, d_loss=0.694, g_loss=0.762\n",
      ">3, 259/273, d_loss=0.700, g_loss=0.769\n",
      ">3, 260/273, d_loss=0.687, g_loss=0.768\n",
      ">3, 261/273, d_loss=0.689, g_loss=0.723\n",
      ">3, 262/273, d_loss=0.663, g_loss=0.744\n",
      ">3, 263/273, d_loss=0.675, g_loss=0.755\n",
      ">3, 264/273, d_loss=0.651, g_loss=0.741\n",
      ">3, 265/273, d_loss=0.656, g_loss=0.732\n",
      ">3, 266/273, d_loss=0.636, g_loss=0.719\n",
      ">3, 267/273, d_loss=0.634, g_loss=0.737\n",
      ">3, 268/273, d_loss=0.644, g_loss=0.760\n",
      ">3, 269/273, d_loss=0.641, g_loss=0.819\n",
      ">3, 270/273, d_loss=0.676, g_loss=0.855\n",
      ">3, 271/273, d_loss=0.684, g_loss=0.833\n",
      ">3, 272/273, d_loss=0.694, g_loss=0.815\n",
      ">3, 273/273, d_loss=0.708, g_loss=0.812\n",
      ">4, 1/273, d_loss=0.698, g_loss=0.755\n",
      ">4, 2/273, d_loss=0.718, g_loss=0.767\n",
      ">4, 3/273, d_loss=0.714, g_loss=0.776\n",
      ">4, 4/273, d_loss=0.716, g_loss=0.777\n",
      ">4, 5/273, d_loss=0.690, g_loss=0.736\n",
      ">4, 6/273, d_loss=0.686, g_loss=0.718\n",
      ">4, 7/273, d_loss=0.675, g_loss=0.712\n",
      ">4, 8/273, d_loss=0.666, g_loss=0.738\n",
      ">4, 9/273, d_loss=0.675, g_loss=0.735\n",
      ">4, 10/273, d_loss=0.676, g_loss=0.714\n",
      ">4, 11/273, d_loss=0.659, g_loss=0.747\n",
      ">4, 12/273, d_loss=0.666, g_loss=0.761\n",
      ">4, 13/273, d_loss=0.672, g_loss=0.775\n",
      ">4, 14/273, d_loss=0.657, g_loss=0.769\n",
      ">4, 15/273, d_loss=0.678, g_loss=0.751\n",
      ">4, 16/273, d_loss=0.655, g_loss=0.774\n",
      ">4, 17/273, d_loss=0.644, g_loss=0.795\n",
      ">4, 18/273, d_loss=0.657, g_loss=0.820\n",
      ">4, 19/273, d_loss=0.653, g_loss=0.847\n",
      ">4, 20/273, d_loss=0.654, g_loss=0.845\n",
      ">4, 21/273, d_loss=0.656, g_loss=0.817\n",
      ">4, 22/273, d_loss=0.667, g_loss=0.869\n",
      ">4, 23/273, d_loss=0.661, g_loss=0.872\n",
      ">4, 24/273, d_loss=0.682, g_loss=0.859\n",
      ">4, 25/273, d_loss=0.682, g_loss=0.827\n",
      ">4, 26/273, d_loss=0.670, g_loss=0.824\n",
      ">4, 27/273, d_loss=0.667, g_loss=0.842\n",
      ">4, 28/273, d_loss=0.666, g_loss=0.817\n",
      ">4, 29/273, d_loss=0.660, g_loss=0.794\n",
      ">4, 30/273, d_loss=0.662, g_loss=0.766\n",
      ">4, 31/273, d_loss=0.668, g_loss=0.783\n",
      ">4, 32/273, d_loss=0.665, g_loss=0.751\n",
      ">4, 33/273, d_loss=0.656, g_loss=0.762\n",
      ">4, 34/273, d_loss=0.675, g_loss=0.770\n",
      ">4, 35/273, d_loss=0.660, g_loss=0.797\n",
      ">4, 36/273, d_loss=0.660, g_loss=0.800\n",
      ">4, 37/273, d_loss=0.676, g_loss=0.802\n",
      ">4, 38/273, d_loss=0.679, g_loss=0.802\n",
      ">4, 39/273, d_loss=0.686, g_loss=0.793\n",
      ">4, 40/273, d_loss=0.674, g_loss=0.803\n",
      ">4, 41/273, d_loss=0.691, g_loss=0.799\n",
      ">4, 42/273, d_loss=0.674, g_loss=0.791\n",
      ">4, 43/273, d_loss=0.666, g_loss=0.824\n",
      ">4, 44/273, d_loss=0.662, g_loss=0.798\n",
      ">4, 45/273, d_loss=0.663, g_loss=0.785\n",
      ">4, 46/273, d_loss=0.655, g_loss=0.786\n",
      ">4, 47/273, d_loss=0.650, g_loss=0.769\n",
      ">4, 48/273, d_loss=0.642, g_loss=0.744\n",
      ">4, 49/273, d_loss=0.647, g_loss=0.762\n",
      ">4, 50/273, d_loss=0.642, g_loss=0.805\n",
      ">4, 51/273, d_loss=0.640, g_loss=0.842\n",
      ">4, 52/273, d_loss=0.651, g_loss=0.842\n",
      ">4, 53/273, d_loss=0.639, g_loss=0.838\n",
      ">4, 54/273, d_loss=0.654, g_loss=0.808\n",
      ">4, 55/273, d_loss=0.661, g_loss=0.763\n",
      ">4, 56/273, d_loss=0.671, g_loss=0.759\n",
      ">4, 57/273, d_loss=0.663, g_loss=0.756\n",
      ">4, 58/273, d_loss=0.687, g_loss=0.732\n",
      ">4, 59/273, d_loss=0.708, g_loss=0.730\n",
      ">4, 60/273, d_loss=0.694, g_loss=0.729\n",
      ">4, 61/273, d_loss=0.686, g_loss=0.737\n",
      ">4, 62/273, d_loss=0.689, g_loss=0.730\n",
      ">4, 63/273, d_loss=0.672, g_loss=0.730\n",
      ">4, 64/273, d_loss=0.661, g_loss=0.730\n",
      ">4, 65/273, d_loss=0.672, g_loss=0.760\n",
      ">4, 66/273, d_loss=0.663, g_loss=0.772\n",
      ">4, 67/273, d_loss=0.656, g_loss=0.773\n",
      ">4, 68/273, d_loss=0.665, g_loss=0.749\n",
      ">4, 69/273, d_loss=0.661, g_loss=0.745\n",
      ">4, 70/273, d_loss=0.657, g_loss=0.715\n",
      ">4, 71/273, d_loss=0.664, g_loss=0.719\n",
      ">4, 72/273, d_loss=0.651, g_loss=0.739\n",
      ">4, 73/273, d_loss=0.667, g_loss=0.684\n",
      ">4, 74/273, d_loss=0.679, g_loss=0.658\n",
      ">4, 75/273, d_loss=0.672, g_loss=0.626\n",
      ">4, 76/273, d_loss=0.681, g_loss=0.613\n",
      ">4, 77/273, d_loss=0.670, g_loss=0.647\n",
      ">4, 78/273, d_loss=0.663, g_loss=0.681\n",
      ">4, 79/273, d_loss=0.688, g_loss=0.724\n",
      ">4, 80/273, d_loss=0.670, g_loss=0.741\n",
      ">4, 81/273, d_loss=0.679, g_loss=0.720\n",
      ">4, 82/273, d_loss=0.662, g_loss=0.698\n",
      ">4, 83/273, d_loss=0.693, g_loss=0.735\n",
      ">4, 84/273, d_loss=0.689, g_loss=0.749\n",
      ">4, 85/273, d_loss=0.665, g_loss=0.761\n",
      ">4, 86/273, d_loss=0.653, g_loss=0.777\n",
      ">4, 87/273, d_loss=0.655, g_loss=0.784\n",
      ">4, 88/273, d_loss=0.642, g_loss=0.785\n",
      ">4, 89/273, d_loss=0.678, g_loss=0.722\n",
      ">4, 90/273, d_loss=0.670, g_loss=0.722\n",
      ">4, 91/273, d_loss=0.652, g_loss=0.730\n",
      ">4, 92/273, d_loss=0.658, g_loss=0.713\n",
      ">4, 93/273, d_loss=0.663, g_loss=0.720\n",
      ">4, 94/273, d_loss=0.665, g_loss=0.733\n",
      ">4, 95/273, d_loss=0.648, g_loss=0.742\n",
      ">4, 96/273, d_loss=0.644, g_loss=0.767\n",
      ">4, 97/273, d_loss=0.655, g_loss=0.794\n",
      ">4, 98/273, d_loss=0.678, g_loss=0.824\n",
      ">4, 99/273, d_loss=0.661, g_loss=0.812\n",
      ">4, 100/273, d_loss=0.666, g_loss=0.807\n",
      ">4, 101/273, d_loss=0.664, g_loss=0.804\n",
      ">4, 102/273, d_loss=0.666, g_loss=0.820\n",
      ">4, 103/273, d_loss=0.676, g_loss=0.842\n",
      ">4, 104/273, d_loss=0.664, g_loss=0.834\n",
      ">4, 105/273, d_loss=0.646, g_loss=0.858\n",
      ">4, 106/273, d_loss=0.660, g_loss=0.839\n",
      ">4, 107/273, d_loss=0.663, g_loss=0.823\n",
      ">4, 108/273, d_loss=0.671, g_loss=0.764\n",
      ">4, 109/273, d_loss=0.668, g_loss=0.771\n",
      ">4, 110/273, d_loss=0.658, g_loss=0.754\n",
      ">4, 111/273, d_loss=0.660, g_loss=0.811\n",
      ">4, 112/273, d_loss=0.671, g_loss=0.800\n",
      ">4, 113/273, d_loss=0.634, g_loss=0.785\n",
      ">4, 114/273, d_loss=0.667, g_loss=0.764\n",
      ">4, 115/273, d_loss=0.658, g_loss=0.750\n",
      ">4, 116/273, d_loss=0.657, g_loss=0.712\n",
      ">4, 117/273, d_loss=0.662, g_loss=0.719\n",
      ">4, 118/273, d_loss=0.671, g_loss=0.735\n",
      ">4, 119/273, d_loss=0.662, g_loss=0.766\n",
      ">4, 120/273, d_loss=0.676, g_loss=0.793\n",
      ">4, 121/273, d_loss=0.663, g_loss=0.817\n",
      ">4, 122/273, d_loss=0.670, g_loss=0.809\n",
      ">4, 123/273, d_loss=0.654, g_loss=0.843\n",
      ">4, 124/273, d_loss=0.658, g_loss=0.805\n",
      ">4, 125/273, d_loss=0.672, g_loss=0.783\n",
      ">4, 126/273, d_loss=0.663, g_loss=0.699\n",
      ">4, 127/273, d_loss=0.634, g_loss=0.726\n",
      ">4, 128/273, d_loss=0.640, g_loss=0.732\n",
      ">4, 129/273, d_loss=0.628, g_loss=0.754\n",
      ">4, 130/273, d_loss=0.645, g_loss=0.730\n",
      ">4, 131/273, d_loss=0.641, g_loss=0.724\n",
      ">4, 132/273, d_loss=0.662, g_loss=0.748\n",
      ">4, 133/273, d_loss=0.669, g_loss=0.770\n",
      ">4, 134/273, d_loss=0.701, g_loss=0.822\n",
      ">4, 135/273, d_loss=0.681, g_loss=0.770\n",
      ">4, 136/273, d_loss=0.696, g_loss=0.763\n",
      ">4, 137/273, d_loss=0.717, g_loss=0.797\n",
      ">4, 138/273, d_loss=0.702, g_loss=0.853\n",
      ">4, 139/273, d_loss=0.690, g_loss=0.830\n",
      ">4, 140/273, d_loss=0.673, g_loss=0.833\n",
      ">4, 141/273, d_loss=0.659, g_loss=0.821\n",
      ">4, 142/273, d_loss=0.664, g_loss=0.816\n",
      ">4, 143/273, d_loss=0.642, g_loss=0.839\n",
      ">4, 144/273, d_loss=0.628, g_loss=0.823\n",
      ">4, 145/273, d_loss=0.642, g_loss=0.844\n",
      ">4, 146/273, d_loss=0.634, g_loss=0.853\n",
      ">4, 147/273, d_loss=0.647, g_loss=0.867\n",
      ">4, 148/273, d_loss=0.634, g_loss=0.833\n",
      ">4, 149/273, d_loss=0.647, g_loss=0.839\n",
      ">4, 150/273, d_loss=0.636, g_loss=0.835\n",
      ">4, 151/273, d_loss=0.667, g_loss=0.857\n",
      ">4, 152/273, d_loss=0.659, g_loss=0.857\n",
      ">4, 153/273, d_loss=0.653, g_loss=0.921\n",
      ">4, 154/273, d_loss=0.650, g_loss=0.925\n",
      ">4, 155/273, d_loss=0.646, g_loss=0.819\n",
      ">4, 156/273, d_loss=0.655, g_loss=0.814\n",
      ">4, 157/273, d_loss=0.648, g_loss=0.827\n",
      ">4, 158/273, d_loss=0.671, g_loss=0.832\n",
      ">4, 159/273, d_loss=0.658, g_loss=0.842\n",
      ">4, 160/273, d_loss=0.656, g_loss=0.782\n",
      ">4, 161/273, d_loss=0.660, g_loss=0.760\n",
      ">4, 162/273, d_loss=0.669, g_loss=0.729\n",
      ">4, 163/273, d_loss=0.646, g_loss=0.721\n",
      ">4, 164/273, d_loss=0.669, g_loss=0.702\n",
      ">4, 165/273, d_loss=0.677, g_loss=0.669\n",
      ">4, 166/273, d_loss=0.674, g_loss=0.653\n",
      ">4, 167/273, d_loss=0.678, g_loss=0.637\n",
      ">4, 168/273, d_loss=0.693, g_loss=0.605\n",
      ">4, 169/273, d_loss=0.686, g_loss=0.605\n",
      ">4, 170/273, d_loss=0.670, g_loss=0.627\n",
      ">4, 171/273, d_loss=0.667, g_loss=0.675\n",
      ">4, 172/273, d_loss=0.660, g_loss=0.710\n",
      ">4, 173/273, d_loss=0.683, g_loss=0.739\n",
      ">4, 174/273, d_loss=0.672, g_loss=0.756\n",
      ">4, 175/273, d_loss=0.653, g_loss=0.708\n",
      ">4, 176/273, d_loss=0.658, g_loss=0.767\n",
      ">4, 177/273, d_loss=0.685, g_loss=0.773\n",
      ">4, 178/273, d_loss=0.658, g_loss=0.785\n",
      ">4, 179/273, d_loss=0.672, g_loss=0.768\n",
      ">4, 180/273, d_loss=0.679, g_loss=0.781\n",
      ">4, 181/273, d_loss=0.676, g_loss=0.773\n",
      ">4, 182/273, d_loss=0.667, g_loss=0.749\n",
      ">4, 183/273, d_loss=0.682, g_loss=0.801\n",
      ">4, 184/273, d_loss=0.662, g_loss=0.834\n",
      ">4, 185/273, d_loss=0.667, g_loss=0.853\n",
      ">4, 186/273, d_loss=0.659, g_loss=0.855\n",
      ">4, 187/273, d_loss=0.665, g_loss=0.883\n",
      ">4, 188/273, d_loss=0.655, g_loss=0.891\n",
      ">4, 189/273, d_loss=0.651, g_loss=0.863\n",
      ">4, 190/273, d_loss=0.656, g_loss=0.794\n",
      ">4, 191/273, d_loss=0.646, g_loss=0.761\n",
      ">4, 192/273, d_loss=0.656, g_loss=0.776\n",
      ">4, 193/273, d_loss=0.661, g_loss=0.754\n",
      ">4, 194/273, d_loss=0.659, g_loss=0.727\n",
      ">4, 195/273, d_loss=0.677, g_loss=0.754\n",
      ">4, 196/273, d_loss=0.668, g_loss=0.749\n",
      ">4, 197/273, d_loss=0.695, g_loss=0.748\n",
      ">4, 198/273, d_loss=0.680, g_loss=0.731\n",
      ">4, 199/273, d_loss=0.677, g_loss=0.710\n",
      ">4, 200/273, d_loss=0.666, g_loss=0.717\n",
      ">4, 201/273, d_loss=0.646, g_loss=0.724\n",
      ">4, 202/273, d_loss=0.646, g_loss=0.695\n",
      ">4, 203/273, d_loss=0.646, g_loss=0.707\n",
      ">4, 204/273, d_loss=0.633, g_loss=0.706\n",
      ">4, 205/273, d_loss=0.648, g_loss=0.717\n",
      ">4, 206/273, d_loss=0.657, g_loss=0.702\n",
      ">4, 207/273, d_loss=0.635, g_loss=0.710\n",
      ">4, 208/273, d_loss=0.645, g_loss=0.741\n",
      ">4, 209/273, d_loss=0.664, g_loss=0.781\n",
      ">4, 210/273, d_loss=0.650, g_loss=0.750\n",
      ">4, 211/273, d_loss=0.661, g_loss=0.752\n",
      ">4, 212/273, d_loss=0.677, g_loss=0.759\n",
      ">4, 213/273, d_loss=0.685, g_loss=0.720\n",
      ">4, 214/273, d_loss=0.671, g_loss=0.746\n",
      ">4, 215/273, d_loss=0.674, g_loss=0.723\n",
      ">4, 216/273, d_loss=0.665, g_loss=0.724\n",
      ">4, 217/273, d_loss=0.664, g_loss=0.766\n",
      ">4, 218/273, d_loss=0.660, g_loss=0.823\n",
      ">4, 219/273, d_loss=0.670, g_loss=0.829\n",
      ">4, 220/273, d_loss=0.647, g_loss=0.792\n",
      ">4, 221/273, d_loss=0.669, g_loss=0.786\n",
      ">4, 222/273, d_loss=0.674, g_loss=0.763\n",
      ">4, 223/273, d_loss=0.673, g_loss=0.758\n",
      ">4, 224/273, d_loss=0.641, g_loss=0.756\n",
      ">4, 225/273, d_loss=0.657, g_loss=0.757\n",
      ">4, 226/273, d_loss=0.651, g_loss=0.787\n",
      ">4, 227/273, d_loss=0.655, g_loss=0.803\n",
      ">4, 228/273, d_loss=0.642, g_loss=0.800\n",
      ">4, 229/273, d_loss=0.637, g_loss=0.803\n",
      ">4, 230/273, d_loss=0.658, g_loss=0.798\n",
      ">4, 231/273, d_loss=0.679, g_loss=0.820\n",
      ">4, 232/273, d_loss=0.674, g_loss=0.772\n",
      ">4, 233/273, d_loss=0.664, g_loss=0.782\n",
      ">4, 234/273, d_loss=0.668, g_loss=0.756\n",
      ">4, 235/273, d_loss=0.655, g_loss=0.789\n",
      ">4, 236/273, d_loss=0.667, g_loss=0.747\n",
      ">4, 237/273, d_loss=0.660, g_loss=0.743\n",
      ">4, 238/273, d_loss=0.674, g_loss=0.739\n",
      ">4, 239/273, d_loss=0.633, g_loss=0.759\n",
      ">4, 240/273, d_loss=0.652, g_loss=0.795\n",
      ">4, 241/273, d_loss=0.661, g_loss=0.798\n",
      ">4, 242/273, d_loss=0.675, g_loss=0.852\n",
      ">4, 243/273, d_loss=0.657, g_loss=0.855\n",
      ">4, 244/273, d_loss=0.649, g_loss=0.787\n",
      ">4, 245/273, d_loss=0.685, g_loss=0.815\n",
      ">4, 246/273, d_loss=0.661, g_loss=0.795\n",
      ">4, 247/273, d_loss=0.668, g_loss=0.783\n",
      ">4, 248/273, d_loss=0.650, g_loss=0.762\n",
      ">4, 249/273, d_loss=0.666, g_loss=0.807\n",
      ">4, 250/273, d_loss=0.657, g_loss=0.793\n",
      ">4, 251/273, d_loss=0.658, g_loss=0.862\n",
      ">4, 252/273, d_loss=0.659, g_loss=0.827\n",
      ">4, 253/273, d_loss=0.664, g_loss=0.813\n",
      ">4, 254/273, d_loss=0.671, g_loss=0.815\n",
      ">4, 255/273, d_loss=0.667, g_loss=0.821\n",
      ">4, 256/273, d_loss=0.644, g_loss=0.794\n",
      ">4, 257/273, d_loss=0.670, g_loss=0.761\n",
      ">4, 258/273, d_loss=0.670, g_loss=0.746\n",
      ">4, 259/273, d_loss=0.660, g_loss=0.711\n",
      ">4, 260/273, d_loss=0.660, g_loss=0.726\n",
      ">4, 261/273, d_loss=0.662, g_loss=0.695\n",
      ">4, 262/273, d_loss=0.673, g_loss=0.703\n",
      ">4, 263/273, d_loss=0.682, g_loss=0.672\n",
      ">4, 264/273, d_loss=0.664, g_loss=0.657\n",
      ">4, 265/273, d_loss=0.661, g_loss=0.663\n",
      ">4, 266/273, d_loss=0.645, g_loss=0.671\n",
      ">4, 267/273, d_loss=0.649, g_loss=0.694\n",
      ">4, 268/273, d_loss=0.649, g_loss=0.721\n",
      ">4, 269/273, d_loss=0.647, g_loss=0.759\n",
      ">4, 270/273, d_loss=0.653, g_loss=0.787\n",
      ">4, 271/273, d_loss=0.652, g_loss=0.789\n",
      ">4, 272/273, d_loss=0.657, g_loss=0.830\n",
      ">4, 273/273, d_loss=0.657, g_loss=0.780\n",
      ">5, 1/273, d_loss=0.656, g_loss=0.771\n",
      ">5, 2/273, d_loss=0.656, g_loss=0.785\n",
      ">5, 3/273, d_loss=0.655, g_loss=0.751\n",
      ">5, 4/273, d_loss=0.644, g_loss=0.761\n",
      ">5, 5/273, d_loss=0.659, g_loss=0.754\n",
      ">5, 6/273, d_loss=0.676, g_loss=0.800\n",
      ">5, 7/273, d_loss=0.693, g_loss=0.807\n",
      ">5, 8/273, d_loss=0.674, g_loss=0.806\n",
      ">5, 9/273, d_loss=0.655, g_loss=0.827\n",
      ">5, 10/273, d_loss=0.653, g_loss=0.852\n",
      ">5, 11/273, d_loss=0.644, g_loss=0.840\n",
      ">5, 12/273, d_loss=0.652, g_loss=0.806\n",
      ">5, 13/273, d_loss=0.670, g_loss=0.762\n",
      ">5, 14/273, d_loss=0.674, g_loss=0.782\n",
      ">5, 15/273, d_loss=0.667, g_loss=0.780\n",
      ">5, 16/273, d_loss=0.664, g_loss=0.761\n",
      ">5, 17/273, d_loss=0.683, g_loss=0.760\n",
      ">5, 18/273, d_loss=0.671, g_loss=0.741\n",
      ">5, 19/273, d_loss=0.674, g_loss=0.692\n",
      ">5, 20/273, d_loss=0.673, g_loss=0.711\n",
      ">5, 21/273, d_loss=0.643, g_loss=0.775\n",
      ">5, 22/273, d_loss=0.670, g_loss=0.811\n",
      ">5, 23/273, d_loss=0.649, g_loss=0.803\n",
      ">5, 24/273, d_loss=0.655, g_loss=0.847\n",
      ">5, 25/273, d_loss=0.659, g_loss=0.904\n",
      ">5, 26/273, d_loss=0.673, g_loss=0.862\n",
      ">5, 27/273, d_loss=0.661, g_loss=0.782\n",
      ">5, 28/273, d_loss=0.658, g_loss=0.746\n",
      ">5, 29/273, d_loss=0.686, g_loss=0.787\n",
      ">5, 30/273, d_loss=0.680, g_loss=0.738\n",
      ">5, 31/273, d_loss=0.652, g_loss=0.743\n",
      ">5, 32/273, d_loss=0.674, g_loss=0.755\n",
      ">5, 33/273, d_loss=0.669, g_loss=0.766\n",
      ">5, 34/273, d_loss=0.659, g_loss=0.791\n",
      ">5, 35/273, d_loss=0.662, g_loss=0.788\n",
      ">5, 36/273, d_loss=0.648, g_loss=0.755\n",
      ">5, 37/273, d_loss=0.666, g_loss=0.805\n",
      ">5, 38/273, d_loss=0.659, g_loss=0.749\n",
      ">5, 39/273, d_loss=0.664, g_loss=0.799\n",
      ">5, 40/273, d_loss=0.658, g_loss=0.830\n",
      ">5, 41/273, d_loss=0.666, g_loss=0.826\n",
      ">5, 42/273, d_loss=0.664, g_loss=0.813\n",
      ">5, 43/273, d_loss=0.669, g_loss=0.773\n",
      ">5, 44/273, d_loss=0.662, g_loss=0.725\n",
      ">5, 45/273, d_loss=0.657, g_loss=0.693\n",
      ">5, 46/273, d_loss=0.648, g_loss=0.678\n",
      ">5, 47/273, d_loss=0.660, g_loss=0.678\n",
      ">5, 48/273, d_loss=0.637, g_loss=0.702\n",
      ">5, 49/273, d_loss=0.650, g_loss=0.769\n",
      ">5, 50/273, d_loss=0.648, g_loss=0.750\n",
      ">5, 51/273, d_loss=0.650, g_loss=0.745\n",
      ">5, 52/273, d_loss=0.645, g_loss=0.794\n",
      ">5, 53/273, d_loss=0.659, g_loss=0.819\n",
      ">5, 54/273, d_loss=0.663, g_loss=0.849\n",
      ">5, 55/273, d_loss=0.665, g_loss=0.799\n",
      ">5, 56/273, d_loss=0.665, g_loss=0.770\n",
      ">5, 57/273, d_loss=0.675, g_loss=0.785\n",
      ">5, 58/273, d_loss=0.664, g_loss=0.800\n",
      ">5, 59/273, d_loss=0.673, g_loss=0.779\n",
      ">5, 60/273, d_loss=0.645, g_loss=0.781\n",
      ">5, 61/273, d_loss=0.655, g_loss=0.766\n",
      ">5, 62/273, d_loss=0.644, g_loss=0.739\n",
      ">5, 63/273, d_loss=0.652, g_loss=0.846\n",
      ">5, 64/273, d_loss=0.652, g_loss=0.909\n",
      ">5, 65/273, d_loss=0.641, g_loss=0.929\n",
      ">5, 66/273, d_loss=0.681, g_loss=0.891\n",
      ">5, 67/273, d_loss=0.658, g_loss=0.860\n",
      ">5, 68/273, d_loss=0.655, g_loss=0.858\n",
      ">5, 69/273, d_loss=0.669, g_loss=0.776\n",
      ">5, 70/273, d_loss=0.676, g_loss=0.733\n",
      ">5, 71/273, d_loss=0.689, g_loss=0.685\n",
      ">5, 72/273, d_loss=0.681, g_loss=0.675\n",
      ">5, 73/273, d_loss=0.708, g_loss=0.676\n",
      ">5, 74/273, d_loss=0.669, g_loss=0.647\n",
      ">5, 75/273, d_loss=0.667, g_loss=0.643\n",
      ">5, 76/273, d_loss=0.663, g_loss=0.676\n",
      ">5, 77/273, d_loss=0.639, g_loss=0.695\n",
      ">5, 78/273, d_loss=0.653, g_loss=0.770\n",
      ">5, 79/273, d_loss=0.659, g_loss=0.892\n",
      ">5, 80/273, d_loss=0.661, g_loss=0.952\n",
      ">5, 81/273, d_loss=0.670, g_loss=0.898\n",
      ">5, 82/273, d_loss=0.688, g_loss=0.847\n",
      ">5, 83/273, d_loss=0.710, g_loss=0.881\n",
      ">5, 84/273, d_loss=0.701, g_loss=0.868\n",
      ">5, 85/273, d_loss=0.706, g_loss=0.812\n",
      ">5, 86/273, d_loss=0.671, g_loss=0.764\n",
      ">5, 87/273, d_loss=0.699, g_loss=0.750\n",
      ">5, 88/273, d_loss=0.660, g_loss=0.721\n",
      ">5, 89/273, d_loss=0.672, g_loss=0.723\n",
      ">5, 90/273, d_loss=0.654, g_loss=0.722\n",
      ">5, 91/273, d_loss=0.661, g_loss=0.731\n",
      ">5, 92/273, d_loss=0.668, g_loss=0.791\n",
      ">5, 93/273, d_loss=0.640, g_loss=0.877\n",
      ">5, 94/273, d_loss=0.670, g_loss=0.908\n",
      ">5, 95/273, d_loss=0.652, g_loss=0.864\n",
      ">5, 96/273, d_loss=0.660, g_loss=0.861\n",
      ">5, 97/273, d_loss=0.636, g_loss=0.798\n",
      ">5, 98/273, d_loss=0.642, g_loss=0.801\n",
      ">5, 99/273, d_loss=0.644, g_loss=0.815\n",
      ">5, 100/273, d_loss=0.649, g_loss=0.830\n",
      ">5, 101/273, d_loss=0.685, g_loss=0.675\n",
      ">5, 102/273, d_loss=0.688, g_loss=0.670\n",
      ">5, 103/273, d_loss=0.667, g_loss=0.690\n",
      ">5, 104/273, d_loss=0.685, g_loss=0.709\n",
      ">5, 105/273, d_loss=0.662, g_loss=0.755\n",
      ">5, 106/273, d_loss=0.678, g_loss=0.847\n",
      ">5, 107/273, d_loss=0.692, g_loss=0.862\n",
      ">5, 108/273, d_loss=0.676, g_loss=0.855\n",
      ">5, 109/273, d_loss=0.667, g_loss=0.882\n",
      ">5, 110/273, d_loss=0.656, g_loss=0.896\n",
      ">5, 111/273, d_loss=0.662, g_loss=0.909\n",
      ">5, 112/273, d_loss=0.656, g_loss=0.883\n",
      ">5, 113/273, d_loss=0.644, g_loss=0.876\n",
      ">5, 114/273, d_loss=0.631, g_loss=0.850\n",
      ">5, 115/273, d_loss=0.633, g_loss=0.863\n",
      ">5, 116/273, d_loss=0.621, g_loss=0.885\n",
      ">5, 117/273, d_loss=0.640, g_loss=0.817\n",
      ">5, 118/273, d_loss=0.660, g_loss=0.713\n",
      ">5, 119/273, d_loss=0.648, g_loss=0.750\n",
      ">5, 120/273, d_loss=0.630, g_loss=0.772\n",
      ">5, 121/273, d_loss=0.634, g_loss=0.790\n",
      ">5, 122/273, d_loss=0.649, g_loss=0.863\n",
      ">5, 123/273, d_loss=0.614, g_loss=0.839\n",
      ">5, 124/273, d_loss=0.668, g_loss=0.825\n",
      ">5, 125/273, d_loss=0.646, g_loss=0.812\n",
      ">5, 126/273, d_loss=0.681, g_loss=0.836\n",
      ">5, 127/273, d_loss=0.684, g_loss=0.836\n",
      ">5, 128/273, d_loss=0.658, g_loss=0.823\n",
      ">5, 129/273, d_loss=0.688, g_loss=0.742\n",
      ">5, 130/273, d_loss=0.704, g_loss=0.700\n",
      ">5, 131/273, d_loss=0.707, g_loss=0.638\n",
      ">5, 132/273, d_loss=0.694, g_loss=0.595\n",
      ">5, 133/273, d_loss=0.670, g_loss=0.609\n",
      ">5, 134/273, d_loss=0.665, g_loss=0.634\n",
      ">5, 135/273, d_loss=0.660, g_loss=0.712\n",
      ">5, 136/273, d_loss=0.637, g_loss=0.756\n",
      ">5, 137/273, d_loss=0.651, g_loss=0.813\n",
      ">5, 138/273, d_loss=0.676, g_loss=0.868\n",
      ">5, 139/273, d_loss=0.682, g_loss=0.930\n",
      ">5, 140/273, d_loss=0.661, g_loss=0.947\n",
      ">5, 141/273, d_loss=0.646, g_loss=0.849\n",
      ">5, 142/273, d_loss=0.654, g_loss=0.848\n",
      ">5, 143/273, d_loss=0.649, g_loss=0.830\n",
      ">5, 144/273, d_loss=0.682, g_loss=0.781\n",
      ">5, 145/273, d_loss=0.694, g_loss=0.674\n",
      ">5, 146/273, d_loss=0.667, g_loss=0.687\n",
      ">5, 147/273, d_loss=0.678, g_loss=0.695\n",
      ">5, 148/273, d_loss=0.664, g_loss=0.727\n",
      ">5, 149/273, d_loss=0.655, g_loss=0.740\n",
      ">5, 150/273, d_loss=0.676, g_loss=0.792\n",
      ">5, 151/273, d_loss=0.669, g_loss=0.803\n",
      ">5, 152/273, d_loss=0.674, g_loss=0.790\n",
      ">5, 153/273, d_loss=0.691, g_loss=0.774\n",
      ">5, 154/273, d_loss=0.687, g_loss=0.779\n",
      ">5, 155/273, d_loss=0.690, g_loss=0.730\n",
      ">5, 156/273, d_loss=0.676, g_loss=0.728\n",
      ">5, 157/273, d_loss=0.677, g_loss=0.729\n",
      ">5, 158/273, d_loss=0.658, g_loss=0.716\n",
      ">5, 159/273, d_loss=0.654, g_loss=0.744\n",
      ">5, 160/273, d_loss=0.639, g_loss=0.708\n",
      ">5, 161/273, d_loss=0.634, g_loss=0.734\n",
      ">5, 162/273, d_loss=0.626, g_loss=0.797\n",
      ">5, 163/273, d_loss=0.651, g_loss=0.862\n",
      ">5, 164/273, d_loss=0.672, g_loss=0.895\n",
      ">5, 165/273, d_loss=0.640, g_loss=0.894\n",
      ">5, 166/273, d_loss=0.672, g_loss=0.816\n",
      ">5, 167/273, d_loss=0.677, g_loss=0.849\n",
      ">5, 168/273, d_loss=0.678, g_loss=0.846\n",
      ">5, 169/273, d_loss=0.667, g_loss=0.858\n",
      ">5, 170/273, d_loss=0.678, g_loss=0.763\n",
      ">5, 171/273, d_loss=0.679, g_loss=0.721\n",
      ">5, 172/273, d_loss=0.652, g_loss=0.688\n",
      ">5, 173/273, d_loss=0.670, g_loss=0.717\n",
      ">5, 174/273, d_loss=0.675, g_loss=0.770\n",
      ">5, 175/273, d_loss=0.632, g_loss=0.828\n",
      ">5, 176/273, d_loss=0.673, g_loss=0.867\n",
      ">5, 177/273, d_loss=0.681, g_loss=0.845\n",
      ">5, 178/273, d_loss=0.684, g_loss=0.783\n",
      ">5, 179/273, d_loss=0.660, g_loss=0.710\n",
      ">5, 180/273, d_loss=0.675, g_loss=0.717\n",
      ">5, 181/273, d_loss=0.688, g_loss=0.709\n",
      ">5, 182/273, d_loss=0.682, g_loss=0.715\n",
      ">5, 183/273, d_loss=0.657, g_loss=0.701\n",
      ">5, 184/273, d_loss=0.646, g_loss=0.714\n",
      ">5, 185/273, d_loss=0.608, g_loss=0.704\n",
      ">5, 186/273, d_loss=0.600, g_loss=0.745\n",
      ">5, 187/273, d_loss=0.660, g_loss=0.845\n",
      ">5, 188/273, d_loss=0.672, g_loss=1.023\n",
      ">5, 189/273, d_loss=0.652, g_loss=0.990\n",
      ">5, 190/273, d_loss=0.674, g_loss=0.835\n",
      ">5, 191/273, d_loss=0.687, g_loss=0.846\n",
      ">5, 192/273, d_loss=0.676, g_loss=0.870\n",
      ">5, 193/273, d_loss=0.663, g_loss=0.844\n",
      ">5, 194/273, d_loss=0.685, g_loss=0.807\n",
      ">5, 195/273, d_loss=0.674, g_loss=0.756\n",
      ">5, 196/273, d_loss=0.668, g_loss=0.722\n",
      ">5, 197/273, d_loss=0.634, g_loss=0.757\n",
      ">5, 198/273, d_loss=0.657, g_loss=0.778\n",
      ">5, 199/273, d_loss=0.635, g_loss=0.787\n",
      ">5, 200/273, d_loss=0.663, g_loss=0.794\n",
      ">5, 201/273, d_loss=0.643, g_loss=0.860\n",
      ">5, 202/273, d_loss=0.659, g_loss=0.932\n",
      ">5, 203/273, d_loss=0.664, g_loss=0.849\n",
      ">5, 204/273, d_loss=0.664, g_loss=0.817\n",
      ">5, 205/273, d_loss=0.662, g_loss=0.848\n",
      ">5, 206/273, d_loss=0.680, g_loss=0.825\n",
      ">5, 207/273, d_loss=0.664, g_loss=0.787\n",
      ">5, 208/273, d_loss=0.660, g_loss=0.778\n",
      ">5, 209/273, d_loss=0.671, g_loss=0.762\n",
      ">5, 210/273, d_loss=0.662, g_loss=0.730\n",
      ">5, 211/273, d_loss=0.664, g_loss=0.709\n",
      ">5, 212/273, d_loss=0.666, g_loss=0.682\n",
      ">5, 213/273, d_loss=0.628, g_loss=0.712\n",
      ">5, 214/273, d_loss=0.650, g_loss=0.760\n",
      ">5, 215/273, d_loss=0.628, g_loss=0.795\n",
      ">5, 216/273, d_loss=0.672, g_loss=0.852\n",
      ">5, 217/273, d_loss=0.665, g_loss=0.909\n",
      ">5, 218/273, d_loss=0.667, g_loss=0.864\n",
      ">5, 219/273, d_loss=0.665, g_loss=0.871\n",
      ">5, 220/273, d_loss=0.670, g_loss=0.797\n",
      ">5, 221/273, d_loss=0.684, g_loss=0.750\n",
      ">5, 222/273, d_loss=0.652, g_loss=0.718\n",
      ">5, 223/273, d_loss=0.664, g_loss=0.710\n",
      ">5, 224/273, d_loss=0.667, g_loss=0.725\n",
      ">5, 225/273, d_loss=0.659, g_loss=0.747\n",
      ">5, 226/273, d_loss=0.666, g_loss=0.743\n",
      ">5, 227/273, d_loss=0.641, g_loss=0.756\n",
      ">5, 228/273, d_loss=0.644, g_loss=0.759\n",
      ">5, 229/273, d_loss=0.637, g_loss=0.771\n",
      ">5, 230/273, d_loss=0.678, g_loss=0.761\n",
      ">5, 231/273, d_loss=0.639, g_loss=0.753\n",
      ">5, 232/273, d_loss=0.642, g_loss=0.764\n",
      ">5, 233/273, d_loss=0.656, g_loss=0.836\n",
      ">5, 234/273, d_loss=0.655, g_loss=0.852\n",
      ">5, 235/273, d_loss=0.652, g_loss=0.811\n",
      ">5, 236/273, d_loss=0.647, g_loss=0.776\n",
      ">5, 237/273, d_loss=0.675, g_loss=0.740\n",
      ">5, 238/273, d_loss=0.662, g_loss=0.749\n",
      ">5, 239/273, d_loss=0.665, g_loss=0.702\n",
      ">5, 240/273, d_loss=0.648, g_loss=0.699\n",
      ">5, 241/273, d_loss=0.643, g_loss=0.675\n",
      ">5, 242/273, d_loss=0.659, g_loss=0.721\n",
      ">5, 243/273, d_loss=0.644, g_loss=0.788\n",
      ">5, 244/273, d_loss=0.650, g_loss=0.784\n",
      ">5, 245/273, d_loss=0.660, g_loss=0.805\n",
      ">5, 246/273, d_loss=0.676, g_loss=0.859\n",
      ">5, 247/273, d_loss=0.654, g_loss=0.924\n",
      ">5, 248/273, d_loss=0.659, g_loss=0.947\n",
      ">5, 249/273, d_loss=0.658, g_loss=0.880\n",
      ">5, 250/273, d_loss=0.660, g_loss=0.778\n",
      ">5, 251/273, d_loss=0.656, g_loss=0.800\n",
      ">5, 252/273, d_loss=0.664, g_loss=0.796\n",
      ">5, 253/273, d_loss=0.648, g_loss=0.773\n",
      ">5, 254/273, d_loss=0.654, g_loss=0.756\n",
      ">5, 255/273, d_loss=0.679, g_loss=0.769\n",
      ">5, 256/273, d_loss=0.646, g_loss=0.757\n",
      ">5, 257/273, d_loss=0.638, g_loss=0.742\n",
      ">5, 258/273, d_loss=0.648, g_loss=0.741\n",
      ">5, 259/273, d_loss=0.654, g_loss=0.767\n",
      ">5, 260/273, d_loss=0.653, g_loss=0.752\n",
      ">5, 261/273, d_loss=0.661, g_loss=0.755\n",
      ">5, 262/273, d_loss=0.669, g_loss=0.739\n",
      ">5, 263/273, d_loss=0.668, g_loss=0.718\n",
      ">5, 264/273, d_loss=0.673, g_loss=0.687\n",
      ">5, 265/273, d_loss=0.651, g_loss=0.778\n",
      ">5, 266/273, d_loss=0.656, g_loss=0.848\n",
      ">5, 267/273, d_loss=0.665, g_loss=0.879\n",
      ">5, 268/273, d_loss=0.639, g_loss=0.851\n",
      ">5, 269/273, d_loss=0.669, g_loss=0.875\n",
      ">5, 270/273, d_loss=0.653, g_loss=0.852\n",
      ">5, 271/273, d_loss=0.635, g_loss=0.761\n",
      ">5, 272/273, d_loss=0.670, g_loss=0.767\n",
      ">5, 273/273, d_loss=0.655, g_loss=0.745\n",
      ">6, 1/273, d_loss=0.679, g_loss=0.692\n",
      ">6, 2/273, d_loss=0.651, g_loss=0.701\n",
      ">6, 3/273, d_loss=0.634, g_loss=0.732\n",
      ">6, 4/273, d_loss=0.656, g_loss=0.801\n",
      ">6, 5/273, d_loss=0.649, g_loss=0.811\n",
      ">6, 6/273, d_loss=0.648, g_loss=0.824\n",
      ">6, 7/273, d_loss=0.657, g_loss=0.860\n",
      ">6, 8/273, d_loss=0.668, g_loss=0.899\n",
      ">6, 9/273, d_loss=0.657, g_loss=0.872\n",
      ">6, 10/273, d_loss=0.683, g_loss=0.754\n",
      ">6, 11/273, d_loss=0.662, g_loss=0.756\n",
      ">6, 12/273, d_loss=0.677, g_loss=0.760\n",
      ">6, 13/273, d_loss=0.705, g_loss=0.693\n",
      ">6, 14/273, d_loss=0.689, g_loss=0.667\n",
      ">6, 15/273, d_loss=0.668, g_loss=0.716\n",
      ">6, 16/273, d_loss=0.663, g_loss=0.731\n",
      ">6, 17/273, d_loss=0.628, g_loss=0.820\n",
      ">6, 18/273, d_loss=0.647, g_loss=0.927\n",
      ">6, 19/273, d_loss=0.647, g_loss=1.065\n",
      ">6, 20/273, d_loss=0.658, g_loss=1.040\n",
      ">6, 21/273, d_loss=0.639, g_loss=0.949\n",
      ">6, 22/273, d_loss=0.668, g_loss=0.916\n",
      ">6, 23/273, d_loss=0.665, g_loss=0.932\n",
      ">6, 24/273, d_loss=0.678, g_loss=0.828\n",
      ">6, 25/273, d_loss=0.669, g_loss=0.760\n",
      ">6, 26/273, d_loss=0.686, g_loss=0.711\n",
      ">6, 27/273, d_loss=0.707, g_loss=0.714\n",
      ">6, 28/273, d_loss=0.687, g_loss=0.714\n",
      ">6, 29/273, d_loss=0.671, g_loss=0.715\n",
      ">6, 30/273, d_loss=0.662, g_loss=0.769\n",
      ">6, 31/273, d_loss=0.654, g_loss=0.819\n",
      ">6, 32/273, d_loss=0.685, g_loss=0.806\n",
      ">6, 33/273, d_loss=0.687, g_loss=0.810\n",
      ">6, 34/273, d_loss=0.679, g_loss=0.787\n",
      ">6, 35/273, d_loss=0.679, g_loss=0.741\n",
      ">6, 36/273, d_loss=0.666, g_loss=0.829\n",
      ">6, 37/273, d_loss=0.645, g_loss=0.869\n",
      ">6, 38/273, d_loss=0.638, g_loss=0.831\n",
      ">6, 39/273, d_loss=0.617, g_loss=0.841\n",
      ">6, 40/273, d_loss=0.668, g_loss=0.805\n",
      ">6, 41/273, d_loss=0.656, g_loss=0.813\n",
      ">6, 42/273, d_loss=0.665, g_loss=0.792\n",
      ">6, 43/273, d_loss=0.642, g_loss=0.814\n",
      ">6, 44/273, d_loss=0.655, g_loss=0.749\n",
      ">6, 45/273, d_loss=0.647, g_loss=0.736\n",
      ">6, 46/273, d_loss=0.644, g_loss=0.720\n",
      ">6, 47/273, d_loss=0.658, g_loss=0.738\n",
      ">6, 48/273, d_loss=0.642, g_loss=0.740\n",
      ">6, 49/273, d_loss=0.642, g_loss=0.807\n",
      ">6, 50/273, d_loss=0.661, g_loss=0.824\n",
      ">6, 51/273, d_loss=0.659, g_loss=0.834\n",
      ">6, 52/273, d_loss=0.666, g_loss=0.752\n",
      ">6, 53/273, d_loss=0.695, g_loss=0.713\n",
      ">6, 54/273, d_loss=0.667, g_loss=0.760\n",
      ">6, 55/273, d_loss=0.663, g_loss=0.733\n",
      ">6, 56/273, d_loss=0.634, g_loss=0.737\n",
      ">6, 57/273, d_loss=0.641, g_loss=0.775\n",
      ">6, 58/273, d_loss=0.658, g_loss=0.812\n",
      ">6, 59/273, d_loss=0.673, g_loss=0.843\n",
      ">6, 60/273, d_loss=0.625, g_loss=0.855\n",
      ">6, 61/273, d_loss=0.667, g_loss=0.842\n",
      ">6, 62/273, d_loss=0.655, g_loss=0.828\n",
      ">6, 63/273, d_loss=0.627, g_loss=0.804\n",
      ">6, 64/273, d_loss=0.635, g_loss=0.761\n",
      ">6, 65/273, d_loss=0.682, g_loss=0.715\n",
      ">6, 66/273, d_loss=0.667, g_loss=0.671\n",
      ">6, 67/273, d_loss=0.671, g_loss=0.689\n",
      ">6, 68/273, d_loss=0.680, g_loss=0.703\n",
      ">6, 69/273, d_loss=0.665, g_loss=0.750\n",
      ">6, 70/273, d_loss=0.648, g_loss=0.751\n",
      ">6, 71/273, d_loss=0.680, g_loss=0.778\n",
      ">6, 72/273, d_loss=0.675, g_loss=0.782\n",
      ">6, 73/273, d_loss=0.661, g_loss=0.798\n",
      ">6, 74/273, d_loss=0.673, g_loss=0.820\n",
      ">6, 75/273, d_loss=0.673, g_loss=0.861\n",
      ">6, 76/273, d_loss=0.663, g_loss=0.818\n",
      ">6, 77/273, d_loss=0.676, g_loss=0.792\n",
      ">6, 78/273, d_loss=0.661, g_loss=0.722\n",
      ">6, 79/273, d_loss=0.659, g_loss=0.669\n",
      ">6, 80/273, d_loss=0.697, g_loss=0.675\n",
      ">6, 81/273, d_loss=0.664, g_loss=0.717\n",
      ">6, 82/273, d_loss=0.661, g_loss=0.690\n",
      ">6, 83/273, d_loss=0.640, g_loss=0.669\n",
      ">6, 84/273, d_loss=0.654, g_loss=0.688\n",
      ">6, 85/273, d_loss=0.664, g_loss=0.889\n",
      ">6, 86/273, d_loss=0.676, g_loss=1.061\n",
      ">6, 87/273, d_loss=0.696, g_loss=0.938\n",
      ">6, 88/273, d_loss=0.666, g_loss=0.724\n",
      ">6, 89/273, d_loss=0.698, g_loss=0.750\n",
      ">6, 90/273, d_loss=0.662, g_loss=0.872\n",
      ">6, 91/273, d_loss=0.685, g_loss=0.871\n",
      ">6, 92/273, d_loss=0.675, g_loss=0.799\n",
      ">6, 93/273, d_loss=0.661, g_loss=0.714\n",
      ">6, 94/273, d_loss=0.663, g_loss=0.708\n",
      ">6, 95/273, d_loss=0.657, g_loss=0.696\n",
      ">6, 96/273, d_loss=0.656, g_loss=0.677\n",
      ">6, 97/273, d_loss=0.664, g_loss=0.743\n",
      ">6, 98/273, d_loss=0.664, g_loss=0.792\n",
      ">6, 99/273, d_loss=0.655, g_loss=0.775\n",
      ">6, 100/273, d_loss=0.689, g_loss=0.796\n",
      ">6, 101/273, d_loss=0.683, g_loss=0.854\n",
      ">6, 102/273, d_loss=0.686, g_loss=0.854\n",
      ">6, 103/273, d_loss=0.662, g_loss=0.824\n",
      ">6, 104/273, d_loss=0.666, g_loss=0.810\n",
      ">6, 105/273, d_loss=0.656, g_loss=0.767\n",
      ">6, 106/273, d_loss=0.674, g_loss=0.719\n",
      ">6, 107/273, d_loss=0.693, g_loss=0.738\n",
      ">6, 108/273, d_loss=0.692, g_loss=0.723\n",
      ">6, 109/273, d_loss=0.661, g_loss=0.736\n",
      ">6, 110/273, d_loss=0.676, g_loss=0.742\n",
      ">6, 111/273, d_loss=0.669, g_loss=0.721\n",
      ">6, 112/273, d_loss=0.661, g_loss=0.757\n",
      ">6, 113/273, d_loss=0.674, g_loss=0.792\n",
      ">6, 114/273, d_loss=0.662, g_loss=0.780\n",
      ">6, 115/273, d_loss=0.685, g_loss=0.715\n",
      ">6, 116/273, d_loss=0.652, g_loss=0.679\n",
      ">6, 117/273, d_loss=0.660, g_loss=0.705\n",
      ">6, 118/273, d_loss=0.685, g_loss=0.799\n",
      ">6, 119/273, d_loss=0.682, g_loss=0.792\n",
      ">6, 120/273, d_loss=0.670, g_loss=0.742\n",
      ">6, 121/273, d_loss=0.690, g_loss=0.724\n",
      ">6, 122/273, d_loss=0.669, g_loss=0.764\n",
      ">6, 123/273, d_loss=0.665, g_loss=0.743\n",
      ">6, 124/273, d_loss=0.687, g_loss=0.778\n",
      ">6, 125/273, d_loss=0.684, g_loss=0.791\n",
      ">6, 126/273, d_loss=0.662, g_loss=0.815\n",
      ">6, 127/273, d_loss=0.663, g_loss=0.821\n",
      ">6, 128/273, d_loss=0.694, g_loss=0.743\n",
      ">6, 129/273, d_loss=0.687, g_loss=0.669\n",
      ">6, 130/273, d_loss=0.693, g_loss=0.662\n",
      ">6, 131/273, d_loss=0.692, g_loss=0.724\n",
      ">6, 132/273, d_loss=0.680, g_loss=0.795\n",
      ">6, 133/273, d_loss=0.680, g_loss=0.793\n",
      ">6, 134/273, d_loss=0.678, g_loss=0.725\n",
      ">6, 135/273, d_loss=0.684, g_loss=0.693\n",
      ">6, 136/273, d_loss=0.682, g_loss=0.713\n",
      ">6, 137/273, d_loss=0.680, g_loss=0.704\n",
      ">6, 138/273, d_loss=0.648, g_loss=0.700\n",
      ">6, 139/273, d_loss=0.685, g_loss=0.733\n",
      ">6, 140/273, d_loss=0.679, g_loss=0.807\n",
      ">6, 141/273, d_loss=0.677, g_loss=0.872\n",
      ">6, 142/273, d_loss=0.683, g_loss=0.839\n",
      ">6, 143/273, d_loss=0.674, g_loss=0.765\n",
      ">6, 144/273, d_loss=0.699, g_loss=0.715\n",
      ">6, 145/273, d_loss=0.684, g_loss=0.699\n",
      ">6, 146/273, d_loss=0.689, g_loss=0.695\n",
      ">6, 147/273, d_loss=0.686, g_loss=0.721\n",
      ">6, 148/273, d_loss=0.686, g_loss=0.749\n",
      ">6, 149/273, d_loss=0.675, g_loss=0.757\n",
      ">6, 150/273, d_loss=0.672, g_loss=0.753\n",
      ">6, 151/273, d_loss=0.679, g_loss=0.702\n",
      ">6, 152/273, d_loss=0.681, g_loss=0.707\n",
      ">6, 153/273, d_loss=0.662, g_loss=0.702\n",
      ">6, 154/273, d_loss=0.674, g_loss=0.752\n",
      ">6, 155/273, d_loss=0.681, g_loss=0.825\n",
      ">6, 156/273, d_loss=0.687, g_loss=0.817\n",
      ">6, 157/273, d_loss=0.681, g_loss=0.769\n",
      ">6, 158/273, d_loss=0.699, g_loss=0.725\n",
      ">6, 159/273, d_loss=0.701, g_loss=0.676\n",
      ">6, 160/273, d_loss=0.669, g_loss=0.666\n",
      ">6, 161/273, d_loss=0.689, g_loss=0.685\n",
      ">6, 162/273, d_loss=0.691, g_loss=0.729\n",
      ">6, 163/273, d_loss=0.670, g_loss=0.756\n",
      ">6, 164/273, d_loss=0.686, g_loss=0.793\n",
      ">6, 165/273, d_loss=0.666, g_loss=0.791\n",
      ">6, 166/273, d_loss=0.683, g_loss=0.769\n",
      ">6, 167/273, d_loss=0.675, g_loss=0.733\n",
      ">6, 168/273, d_loss=0.668, g_loss=0.719\n",
      ">6, 169/273, d_loss=0.661, g_loss=0.791\n",
      ">6, 170/273, d_loss=0.691, g_loss=0.768\n",
      ">6, 171/273, d_loss=0.669, g_loss=0.753\n",
      ">6, 172/273, d_loss=0.672, g_loss=0.698\n",
      ">6, 173/273, d_loss=0.676, g_loss=0.651\n",
      ">6, 174/273, d_loss=0.666, g_loss=0.671\n",
      ">6, 175/273, d_loss=0.681, g_loss=0.725\n",
      ">6, 176/273, d_loss=0.676, g_loss=0.794\n",
      ">6, 177/273, d_loss=0.685, g_loss=0.821\n",
      ">6, 178/273, d_loss=0.680, g_loss=0.806\n",
      ">6, 179/273, d_loss=0.669, g_loss=0.747\n",
      ">6, 180/273, d_loss=0.682, g_loss=0.726\n",
      ">6, 181/273, d_loss=0.686, g_loss=0.740\n",
      ">6, 182/273, d_loss=0.670, g_loss=0.681\n",
      ">6, 183/273, d_loss=0.682, g_loss=0.692\n",
      ">6, 184/273, d_loss=0.676, g_loss=0.718\n",
      ">6, 185/273, d_loss=0.687, g_loss=0.777\n",
      ">6, 186/273, d_loss=0.682, g_loss=0.814\n",
      ">6, 187/273, d_loss=0.665, g_loss=0.790\n",
      ">6, 188/273, d_loss=0.635, g_loss=0.767\n",
      ">6, 189/273, d_loss=0.681, g_loss=0.752\n",
      ">6, 190/273, d_loss=0.652, g_loss=0.718\n",
      ">6, 191/273, d_loss=0.669, g_loss=0.718\n",
      ">6, 192/273, d_loss=0.682, g_loss=0.760\n",
      ">6, 193/273, d_loss=0.654, g_loss=0.821\n",
      ">6, 194/273, d_loss=0.676, g_loss=0.781\n",
      ">6, 195/273, d_loss=0.655, g_loss=0.704\n",
      ">6, 196/273, d_loss=0.670, g_loss=0.710\n",
      ">6, 197/273, d_loss=0.664, g_loss=0.670\n",
      ">6, 198/273, d_loss=0.681, g_loss=0.680\n",
      ">6, 199/273, d_loss=0.691, g_loss=0.785\n",
      ">6, 200/273, d_loss=0.686, g_loss=0.865\n",
      ">6, 201/273, d_loss=0.674, g_loss=0.823\n",
      ">6, 202/273, d_loss=0.675, g_loss=0.761\n",
      ">6, 203/273, d_loss=0.689, g_loss=0.734\n",
      ">6, 204/273, d_loss=0.677, g_loss=0.723\n",
      ">6, 205/273, d_loss=0.665, g_loss=0.713\n",
      ">6, 206/273, d_loss=0.676, g_loss=0.770\n",
      ">6, 207/273, d_loss=0.677, g_loss=0.792\n",
      ">6, 208/273, d_loss=0.685, g_loss=0.792\n",
      ">6, 209/273, d_loss=0.678, g_loss=0.770\n",
      ">6, 210/273, d_loss=0.667, g_loss=0.692\n",
      ">6, 211/273, d_loss=0.664, g_loss=0.665\n",
      ">6, 212/273, d_loss=0.670, g_loss=0.686\n",
      ">6, 213/273, d_loss=0.688, g_loss=0.722\n",
      ">6, 214/273, d_loss=0.677, g_loss=0.752\n",
      ">6, 215/273, d_loss=0.671, g_loss=0.831\n",
      ">6, 216/273, d_loss=0.693, g_loss=0.811\n",
      ">6, 217/273, d_loss=0.675, g_loss=0.748\n",
      ">6, 218/273, d_loss=0.671, g_loss=0.677\n",
      ">6, 219/273, d_loss=0.685, g_loss=0.694\n",
      ">6, 220/273, d_loss=0.670, g_loss=0.757\n",
      ">6, 221/273, d_loss=0.678, g_loss=0.755\n",
      ">6, 222/273, d_loss=0.669, g_loss=0.747\n",
      ">6, 223/273, d_loss=0.668, g_loss=0.760\n",
      ">6, 224/273, d_loss=0.691, g_loss=0.759\n",
      ">6, 225/273, d_loss=0.680, g_loss=0.776\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "# train model\n",
    "trained_generator = train(generator, discriminator, gan_model, x, latent_dim, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    " \n",
    "# create and display a plot of generated images (reversed grayscale)\n",
    "def display_plot(examples, n):\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "    plt.show()\n",
    "\n",
    "# load model\n",
    "#model = load_model('generator_model_100.h5') #load the last seralized model (latest version of the GAN model)\n",
    "# generate images\n",
    "latent_points = generate_latent_points(100, 25)\n",
    "# generate images\n",
    "X = trained_generator.predict(latent_points)\n",
    "# plot the result\n",
    "display_plot(X, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
